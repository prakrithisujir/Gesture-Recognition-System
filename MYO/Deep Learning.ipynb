{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a19fb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c86b8a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'adafruit_servokit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24584/682506917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0madafruit_servokit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mServoKit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mkit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mServoKit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'adafruit_servokit'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from math import exp\n",
    "import os\n",
    "import glob\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import matplotlib.pyplot as plt #used for visualization purposes in this tutorial.\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, Flatten,Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from adafruit_servokit import ServoKit\n",
    "#kit=ServoKit(Channels=16)\n",
    "\n",
    "csvs_rest = glob.glob(f\"./Datasets/{name}/rest/*.csv\")\n",
    "csvs_index = glob.glob(f\"./Datasets/{name}/index/*.csv\")\n",
    "csvs_closed = glob.glob(f\"./Datasets/{name}/closed/*.csv\")\n",
    "csvs_middle_finger = glob.glob(f\"./Datasets/{name}/middle-finger/*.csv\")\n",
    "#csvs_cylindrical = glob.glob(f\"./Datasets/{name}/cylindrical/*.csv\")\n",
    "csvs_pinch = glob.glob(f\"./Datasets/{name}/pinch/*.csv\")\n",
    "\n",
    "df_rest = pd.DataFrame()\n",
    "df_index = pd.DataFrame()\n",
    "df_closed = pd.DataFrame()\n",
    "df_middle_finger = pd.DataFrame()\n",
    "#df_cylindrical_bottle = pd.DataFrame()\n",
    "df_pinch = pd.DataFrame()\n",
    "for c in csvs_rest:\n",
    "    if df_rest.empty:\n",
    "        df_rest = pd.read_csv(c)\n",
    "    else:\n",
    "        df_rest = pd.concat([df_rest,pd.read_csv(c)]).reset_index(drop=True)\n",
    "        \n",
    "for c in csvs_index:\n",
    "    if df_index.empty:\n",
    "        df_index = pd.read_csv(c)\n",
    "    else:\n",
    "        df_index = pd.concat([df_index,pd.read_csv(c)]).reset_index(drop=True)\n",
    "        \n",
    "for c in csvs_closed:\n",
    "    if df_closed.empty:\n",
    "        df_closed = pd.read_csv(c)\n",
    "    else:\n",
    "        df_closed = pd.concat([df_closed,pd.read_csv(c)]).reset_index(drop=True)\n",
    "        \n",
    "for c in csvs_middle_finger:\n",
    "    if df_middle_finger.empty:\n",
    "        df_middle_finger = pd.read_csv(c)\n",
    "    else:\n",
    "        df_middle_finger = pd.concat([df_middle_finger,pd.read_csv(c)]).reset_index(drop=True)\n",
    "        \n",
    "#for c in csvs_cylindrical:\n",
    " #   if df_cylindrical_bottle.empty:\n",
    "  #      df_cylindrical_bottle = pd.read_csv(c)\n",
    "   # else:\n",
    "    #    df_cylindrical_bottle = pd.concat([df_cylindrical_bottle,pd.read_csv(c)]).reset_index(drop=True)\n",
    "\n",
    "for c in csvs_pinch:\n",
    "    if df_pinch.empty:\n",
    "        df_pinch = pd.read_csv(c)\n",
    "    else:\n",
    "        df_pinch = pd.concat([df_pinch,pd.read_csv(c)]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "index_emg1=df_index.iloc[0:,1]\n",
    "index_emg3=df_index.iloc[0:,3]\n",
    "index_emg8=df_index.iloc[0:,8]\n",
    "\n",
    "middle_emg1=df_middle_finger.iloc[0:,1]\n",
    "middle_emg3=df_middle_finger.iloc[0:,3]\n",
    "middle_emg8=df_middle_finger.iloc[0:,8]\n",
    "\n",
    "closed_emg1=df_closed.iloc[0:,1]\n",
    "closed_emg3=df_closed.iloc[0:,3]\n",
    "closed_emg8=df_closed.iloc[0:,8]\n",
    "\n",
    "#cylindrical_emg1=df_cylindrical_bottle.iloc[0:,1]\n",
    "#cylindrical_emg3=df_cylindrical_bottle.iloc[0:,3]\n",
    "#cylindrical_emg8=df_cylindrical_bottle.iloc[0:,8]\n",
    "\n",
    "rest_emg1=df_rest.iloc[0:,1]\n",
    "rest_emg3=df_rest.iloc[0:,3]\n",
    "rest_emg8=df_rest.iloc[0:,8]\n",
    "\n",
    "pinch_emg1=df_pinch.iloc[0:,1]\n",
    "pinch_emg3=df_pinch.iloc[0:,3]\n",
    "pinch_emg8=df_pinch.iloc[0:,8]\n",
    "\n",
    "\n",
    "index=[]\n",
    "middle=[]\n",
    "closed=[]\n",
    "#cylinder=[]\n",
    "rest=[]\n",
    "pinch=[]\n",
    "\n",
    "index.append([index_emg1])\n",
    "index.append([index_emg3])\n",
    "index.append([index_emg8])\n",
    "\n",
    "middle.append([middle_emg1])\n",
    "middle.append([middle_emg3])\n",
    "middle.append([middle_emg8])\n",
    "\n",
    "closed.append([closed_emg1])\n",
    "closed.append([closed_emg3])\n",
    "closed.append([closed_emg8])\n",
    "\n",
    "#cylinder.append([cylindrical_emg1])\n",
    "#cylinder.append([cylindrical_emg3])\n",
    "#cylinder.append([cylindrical_emg8])\n",
    "\n",
    "pinch.append([pinch_emg1])\n",
    "pinch.append([pinch_emg3])\n",
    "pinch.append([pinch_emg8])\n",
    "\n",
    "rest.append([rest_emg1])\n",
    "rest.append([rest_emg3])\n",
    "rest.append([rest_emg8])\n",
    "\n",
    "gestures=[]\n",
    "gestures.append(index)\n",
    "gestures.append(middle)\n",
    "gestures.append(closed)\n",
    "#gestures.append(cylinder)\n",
    "gestures.append(pinch)\n",
    "gestures.append(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbbb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 200 #samplerate in Hz\n",
    "eventLength =8\n",
    "\n",
    "events=[[\"0\",1],['1',11],['2',21],['3',31],['4',41],['5',51],\n",
    "          ['6',63],['7',73],['8',83],['9',93],['10',102],['11',111],\n",
    "          ['12',124],['13',134],['14',144],['15',154],['16',164],['17',174],\n",
    "          ['18',184],['19',194],['20',204],['21',214],['22',225],['23',235],\n",
    "          ['24',245],['25',255],['26',265],['27',275],['28',285],['29',295],\n",
    "          ['30',307],['31',317],['32',327],['33',336],['34',346],['35',356],\n",
    "          ['36',368],['37',378],['38',388],['39',398],['40',408],['41',418],\n",
    "          ['42',428],['43',438],['44',449],['45',459],['46',469],['47',479],\n",
    "          ['48',489],['49',499],['50',509],['51',519],['52',529],['53',539],\n",
    "          ['54',550],['55',560],['56',570],['57',580],['58',591],['59',601],\n",
    "          ['60',611],['61',621],['62',632],['63',642],['64',652],['65',662],\n",
    "          ['66',673],['67',683],['68',693],['69',703],['70',713],['71',723]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bd0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=[]\n",
    "y0=[]\n",
    "EMG =gestures[0][0][0]\n",
    "label=0\n",
    "windows=[]\n",
    "for event in events:\n",
    "    startSample = sr * event[1] #samplerate of the signal multiplied by the onset of the event in s\n",
    "    endSample = startSample + (sr * eventLength) #Final sample to use for estimation\n",
    "    window=EMG[startSample:endSample]\n",
    "    x0.append(window)\n",
    "    y0.append(label)\n",
    "\n",
    "x1=[]\n",
    "y1=[]\n",
    "EMG =gestures[1][0][0]\n",
    "label=1\n",
    "windows=[]\n",
    "for event in events:\n",
    "    startSample = sr * event[1] #samplerate of the signal multiplied by the onset of the event in s\n",
    "    endSample = startSample + (sr * eventLength) #Final sample to use for estimation\n",
    "    window=EMG[startSample:endSample]\n",
    "    x1.append(window)\n",
    "    y1.append(label)\n",
    "\n",
    "x2=[]\n",
    "y2=[]\n",
    "EMG =gestures[2][0][0]\n",
    "label=2\n",
    "windows=[]\n",
    "for event in events:\n",
    "    startSample = sr * event[1] #samplerate of the signal multiplied by the onset of the event in s\n",
    "    endSample = startSample + (sr * eventLength) #Final sample to use for estimation\n",
    "    window=EMG[startSample:endSample]\n",
    "    x2.append(window)\n",
    "    y2.append(label)\n",
    "\n",
    "x3=[]\n",
    "y3=[]\n",
    "EMG =gestures[3][0][0]\n",
    "label=3\n",
    "windows=[]\n",
    "for event in events:\n",
    "    startSample = sr * event[1] #samplerate of the signal multiplied by the onset of the event in s\n",
    "    endSample = startSample + (sr * eventLength) #Final sample to use for estimation\n",
    "    window=EMG[startSample:endSample]\n",
    "    x3.append(window)\n",
    "    y3.append(label)\n",
    "\n",
    "x4=[]\n",
    "y4=[]\n",
    "EMG =gestures[4][0][0]\n",
    "label=4\n",
    "windows=[]\n",
    "for event in events:\n",
    "    startSample = sr * event[1] #samplerate of the signal multiplied by the onset of the event in s\n",
    "    endSample = startSample + (sr * eventLength) #Final sample to use for estimation\n",
    "    window=EMG[startSample:endSample]\n",
    "    x4.append(window)\n",
    "    y4.append(label)\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "x.append(x0)\n",
    "x.append(x1)\n",
    "x.append(x2)\n",
    "x.append(x3)\n",
    "x.append(x4)\n",
    "\n",
    "y.append(y0)\n",
    "y.append(y1)\n",
    "y.append(y2)\n",
    "y.append(y3)\n",
    "y.append(y4)\n",
    "\n",
    "#GET ALL GESTURES AND LABELS IN THIS FORM\n",
    "X=[]\n",
    "Y=[]\n",
    "for window in x[0]:\n",
    "    X.append(window)\n",
    "for window in x[1]:\n",
    "    X.append(window)\n",
    "for window in x[2]:\n",
    "    X.append(window)\n",
    "for window in x[3]:\n",
    "    X.append(window)\n",
    "for window in x[4]:\n",
    "    X.append(window)\n",
    "\n",
    "for window in y[0]:\n",
    "    Y.append(window)\n",
    "for window in y[1]:\n",
    "    Y.append(window)\n",
    "for window in y[2]:\n",
    "    Y.append(window)\n",
    "for window in y[3]:\n",
    "    Y.append(window)\n",
    "for window in y[4]:\n",
    "    Y.append(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a661be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=5\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.10,random_state=42)\n",
    "#y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "#y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "import numpy as np\n",
    "x_train1=np.asarray(x_train)\n",
    "x_test1=np.asarray(x_test)\n",
    "y_train1=np.asarray(y_train)\n",
    "y_test1=np.asarray(y_test)\n",
    "\n",
    "#x_train1=x_train1.reshape(162, 1600,1)\n",
    "#x_test1=x_test1.reshape(162, 1600, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfec047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.DataFrame(x_train1)\n",
    "df_test=pd.DataFrame(x_test1)\n",
    "df_train['label']=y_train\n",
    "df_test['label']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6982ddce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1591</th>\n",
       "      <th>1592</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "      <th>1599</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "      <td>-27</td>\n",
       "      <td>14</td>\n",
       "      <td>-5</td>\n",
       "      <td>9</td>\n",
       "      <td>-35</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1   2   3  4  5   6  7  8  9  ...  1591  1592  1593  1594  1595  1596  \\\n",
       "0    0 -1  -1   0  0 -3  -1  0  0  0  ...     0     2    -1     0     0    -1   \n",
       "1   -1  0   0   0  0  2  -1 -2  0 -1  ...    -1    -2    -2    -1     0    -2   \n",
       "2    1 -1  -1   0 -1  0  -2  0 -1  0  ...    -1    -2    -2     2     0     0   \n",
       "3    0 -1  -1  -1  0 -2  -2  0 -1  1  ...     1    -1     0    -1    -1     0   \n",
       "4    0 -1   0   0 -1 -1  -1 -2 -1 -2  ...     2     0    -1    -1     1     0   \n",
       "..  .. ..  ..  .. .. ..  .. .. .. ..  ...   ...   ...   ...   ...   ...   ...   \n",
       "355 -1 -1  -1   0 -1 -1   0  0  0 -1  ...    -2     1    -2    -3    -1    -1   \n",
       "356  2 -4   1  -1  1 -3  -1 -2 -5  1  ...    -1     1     0    -5     0     0   \n",
       "357 -2  1  -2   0  0 -1  -2 -1 -1 -1  ...    -1    -1    -2    -2     1     1   \n",
       "358 -3  4 -27  14 -5  9 -35  9  5  1  ...     1    -1    -1    -2     0    -1   \n",
       "359  0  0   0  -3  1 -2   0 -1 -1  1  ...     0    -1    -3     0    -1    -1   \n",
       "\n",
       "     1597  1598  1599  label  \n",
       "0      -1    -2    -2      0  \n",
       "1      -2     0    -1      4  \n",
       "2      -1    -1    -3      1  \n",
       "3      -2    -2    -1      1  \n",
       "4       4     1    -2      2  \n",
       "..    ...   ...   ...    ...  \n",
       "355    -2     1     0      0  \n",
       "356    -3    -3    -2      4  \n",
       "357    -3     0     1      2  \n",
       "358     1    -1     1      1  \n",
       "359    -1    -2     0      0  \n",
       "\n",
       "[360 rows x 1601 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3515392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to CSV\n",
    "df.to_csv('GESTURE_DATAFRAME_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db94e53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1600)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.10,random_state=42)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1952d839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1600, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "x = x.reshape((x.shape[0], x.shape[1], n_features))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1019b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33a8ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1600, 32)          64        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 800, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 800, 64)           2112      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 400, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 400, 32)           2080      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 200, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 200, 16)           528       \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 100, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 16)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 100, 8)            136       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 50, 8)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50, 8)             0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 50, 4)             36        \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 25, 4)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 25, 4)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praahas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning:\n",
      "\n",
      "The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    " \n",
    "keras.regularizers.l1(0.01)\n",
    "keras.regularizers.l2(0.01)\n",
    "keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(x.shape[1], n_features)))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "#model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "#model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu',))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=8, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=4, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d02581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "11/11 [==============================] - 8s 50ms/step - loss: 3.3484 - accuracy: 0.2191 - val_loss: 1.6633 - val_accuracy: 0.1944\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.7628 - accuracy: 0.2623 - val_loss: 1.5613 - val_accuracy: 0.1944\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.6131 - accuracy: 0.2253 - val_loss: 1.5301 - val_accuracy: 0.3056\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.4648 - accuracy: 0.3302 - val_loss: 1.5184 - val_accuracy: 0.3333\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.4056 - accuracy: 0.4444 - val_loss: 1.4977 - val_accuracy: 0.3056\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.3681 - accuracy: 0.4568 - val_loss: 1.4171 - val_accuracy: 0.4722\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.3982 - accuracy: 0.4167 - val_loss: 1.3973 - val_accuracy: 0.4444\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.3350 - accuracy: 0.4660 - val_loss: 1.4388 - val_accuracy: 0.4722\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.3039 - accuracy: 0.4815 - val_loss: 1.3198 - val_accuracy: 0.4722\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.2406 - accuracy: 0.5216 - val_loss: 1.3191 - val_accuracy: 0.5556\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.2330 - accuracy: 0.5340 - val_loss: 1.3048 - val_accuracy: 0.5278\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.1860 - accuracy: 0.5463 - val_loss: 1.2498 - val_accuracy: 0.5556\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.1673 - accuracy: 0.5741 - val_loss: 1.1530 - val_accuracy: 0.6111\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.1241 - accuracy: 0.6358 - val_loss: 1.1413 - val_accuracy: 0.6389\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.1216 - accuracy: 0.6142 - val_loss: 1.0922 - val_accuracy: 0.7222\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.0779 - accuracy: 0.6698 - val_loss: 1.0921 - val_accuracy: 0.6944\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.0519 - accuracy: 0.6358 - val_loss: 1.0062 - val_accuracy: 0.7222\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.0433 - accuracy: 0.6327 - val_loss: 0.9632 - val_accuracy: 0.7500\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.9241 - accuracy: 0.6698 - val_loss: 0.9644 - val_accuracy: 0.6111\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.9569 - accuracy: 0.6451 - val_loss: 0.8669 - val_accuracy: 0.6389\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.8755 - accuracy: 0.6728 - val_loss: 0.8105 - val_accuracy: 0.6389\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.8149 - accuracy: 0.6883 - val_loss: 0.7106 - val_accuracy: 0.7222\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7708 - accuracy: 0.6883 - val_loss: 0.7174 - val_accuracy: 0.6944\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7414 - accuracy: 0.6944 - val_loss: 0.6515 - val_accuracy: 0.7222\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6989 - accuracy: 0.6852 - val_loss: 0.6449 - val_accuracy: 0.6944\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7017 - accuracy: 0.7130 - val_loss: 0.6451 - val_accuracy: 0.7778\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6393 - accuracy: 0.7346 - val_loss: 0.6186 - val_accuracy: 0.7500\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7707 - accuracy: 0.6543 - val_loss: 0.6425 - val_accuracy: 0.7778\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.7142 - accuracy: 0.7284 - val_loss: 0.6634 - val_accuracy: 0.6667\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6766 - accuracy: 0.7160 - val_loss: 0.7032 - val_accuracy: 0.6944\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6508 - accuracy: 0.7037 - val_loss: 0.6874 - val_accuracy: 0.6667\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6564 - accuracy: 0.7315 - val_loss: 0.6701 - val_accuracy: 0.7222\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.6151 - accuracy: 0.7407 - val_loss: 0.6418 - val_accuracy: 0.7500\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5712 - accuracy: 0.7654 - val_loss: 0.6004 - val_accuracy: 0.6667\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6180 - accuracy: 0.7191 - val_loss: 0.5634 - val_accuracy: 0.7222\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5486 - accuracy: 0.7716 - val_loss: 0.5252 - val_accuracy: 0.7778\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5285 - accuracy: 0.7500 - val_loss: 0.5029 - val_accuracy: 0.8056\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5172 - accuracy: 0.7901 - val_loss: 0.5082 - val_accuracy: 0.7778\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5596 - accuracy: 0.7778 - val_loss: 0.5132 - val_accuracy: 0.8056\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.6243 - accuracy: 0.7037 - val_loss: 0.5448 - val_accuracy: 0.7778\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5355 - accuracy: 0.7500 - val_loss: 0.5275 - val_accuracy: 0.8333\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5717 - accuracy: 0.7531 - val_loss: 0.5240 - val_accuracy: 0.8056\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5402 - accuracy: 0.7469 - val_loss: 0.5108 - val_accuracy: 0.7500\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5551 - accuracy: 0.7531 - val_loss: 0.5042 - val_accuracy: 0.8056\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.5139 - accuracy: 0.7623 - val_loss: 0.5007 - val_accuracy: 0.7778\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4874 - accuracy: 0.7901 - val_loss: 0.5157 - val_accuracy: 0.7500\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4671 - accuracy: 0.8025 - val_loss: 0.5016 - val_accuracy: 0.7778\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.5375 - accuracy: 0.7562 - val_loss: 0.5392 - val_accuracy: 0.7500\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.5214 - accuracy: 0.7809 - val_loss: 0.4978 - val_accuracy: 0.7500\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4383 - accuracy: 0.8086 - val_loss: 0.4883 - val_accuracy: 0.7778\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4883 - accuracy: 0.7685 - val_loss: 0.4810 - val_accuracy: 0.7778\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4642 - accuracy: 0.7840 - val_loss: 0.4659 - val_accuracy: 0.7778\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.4396 - accuracy: 0.7963 - val_loss: 0.4696 - val_accuracy: 0.7778\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4421 - accuracy: 0.7901 - val_loss: 0.4649 - val_accuracy: 0.8056\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4671 - accuracy: 0.7870 - val_loss: 0.4688 - val_accuracy: 0.7778\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4144 - accuracy: 0.8025 - val_loss: 0.4703 - val_accuracy: 0.7500\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4467 - accuracy: 0.7809 - val_loss: 0.4562 - val_accuracy: 0.7778\n",
      "Epoch 58/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4170 - accuracy: 0.7932 - val_loss: 0.4612 - val_accuracy: 0.8056\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3937 - accuracy: 0.8333 - val_loss: 0.4551 - val_accuracy: 0.7778\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4295 - accuracy: 0.8272 - val_loss: 0.4448 - val_accuracy: 0.8056\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4325 - accuracy: 0.8148 - val_loss: 0.4312 - val_accuracy: 0.7500\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4060 - accuracy: 0.8148 - val_loss: 0.4410 - val_accuracy: 0.8056\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4616 - accuracy: 0.7778 - val_loss: 0.4635 - val_accuracy: 0.7500\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4153 - accuracy: 0.7932 - val_loss: 0.4354 - val_accuracy: 0.8056\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4713 - accuracy: 0.8025 - val_loss: 0.4323 - val_accuracy: 0.8333\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4310 - accuracy: 0.7778 - val_loss: 0.4321 - val_accuracy: 0.8056\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4301 - accuracy: 0.8056 - val_loss: 0.4320 - val_accuracy: 0.8056\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4273 - accuracy: 0.8117 - val_loss: 0.4291 - val_accuracy: 0.8611\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3597 - accuracy: 0.8086 - val_loss: 0.4436 - val_accuracy: 0.7778\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4381 - accuracy: 0.8117 - val_loss: 0.4215 - val_accuracy: 0.8333\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4284 - accuracy: 0.8148 - val_loss: 0.4691 - val_accuracy: 0.7778\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4003 - accuracy: 0.8272 - val_loss: 0.4136 - val_accuracy: 0.8611\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4005 - accuracy: 0.8148 - val_loss: 0.4340 - val_accuracy: 0.7778\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3679 - accuracy: 0.8333 - val_loss: 0.4244 - val_accuracy: 0.7778\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3661 - accuracy: 0.8272 - val_loss: 0.4175 - val_accuracy: 0.8056\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3982 - accuracy: 0.7994 - val_loss: 0.4175 - val_accuracy: 0.7778\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3762 - accuracy: 0.8241 - val_loss: 0.4114 - val_accuracy: 0.8056\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4108 - accuracy: 0.8395 - val_loss: 0.4080 - val_accuracy: 0.8333\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3706 - accuracy: 0.8395 - val_loss: 0.4464 - val_accuracy: 0.7500\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3353 - accuracy: 0.8642 - val_loss: 0.4130 - val_accuracy: 0.8056\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3904 - accuracy: 0.8148 - val_loss: 0.4119 - val_accuracy: 0.7500\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4184 - accuracy: 0.8086 - val_loss: 0.4018 - val_accuracy: 0.7778\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3646 - accuracy: 0.8179 - val_loss: 0.4015 - val_accuracy: 0.7500\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3821 - accuracy: 0.8148 - val_loss: 0.4017 - val_accuracy: 0.7778\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3569 - accuracy: 0.8488 - val_loss: 0.3951 - val_accuracy: 0.7778\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3851 - accuracy: 0.8086 - val_loss: 0.3954 - val_accuracy: 0.8333\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3395 - accuracy: 0.8426 - val_loss: 0.4118 - val_accuracy: 0.7500\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3615 - accuracy: 0.8426 - val_loss: 0.3790 - val_accuracy: 0.7778\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4070 - accuracy: 0.8241 - val_loss: 0.4760 - val_accuracy: 0.7778\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4288 - accuracy: 0.8179 - val_loss: 0.4137 - val_accuracy: 0.8333\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3608 - accuracy: 0.8426 - val_loss: 0.4894 - val_accuracy: 0.7778\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3833 - accuracy: 0.8364 - val_loss: 0.3977 - val_accuracy: 0.8333\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3576 - accuracy: 0.8302 - val_loss: 0.4210 - val_accuracy: 0.7778\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3688 - accuracy: 0.8395 - val_loss: 0.3993 - val_accuracy: 0.7778\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3374 - accuracy: 0.8302 - val_loss: 0.4678 - val_accuracy: 0.7778\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4035 - accuracy: 0.8086 - val_loss: 0.4062 - val_accuracy: 0.7778\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3745 - accuracy: 0.8241 - val_loss: 0.3892 - val_accuracy: 0.7778\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3344 - accuracy: 0.8395 - val_loss: 0.3945 - val_accuracy: 0.8056\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3491 - accuracy: 0.8210 - val_loss: 0.3917 - val_accuracy: 0.7778\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3410 - accuracy: 0.8272 - val_loss: 0.3868 - val_accuracy: 0.8333\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3975 - accuracy: 0.8210 - val_loss: 0.3977 - val_accuracy: 0.7500\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3153 - accuracy: 0.8426 - val_loss: 0.3921 - val_accuracy: 0.8056\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3357 - accuracy: 0.8272 - val_loss: 0.3909 - val_accuracy: 0.7500\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3466 - accuracy: 0.8457 - val_loss: 0.3941 - val_accuracy: 0.8333\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3356 - accuracy: 0.8488 - val_loss: 0.3959 - val_accuracy: 0.8056\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3188 - accuracy: 0.8488 - val_loss: 0.4166 - val_accuracy: 0.8056\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3139 - accuracy: 0.8519 - val_loss: 0.3974 - val_accuracy: 0.8056\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3173 - accuracy: 0.8735 - val_loss: 0.4199 - val_accuracy: 0.7500\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3649 - accuracy: 0.8241 - val_loss: 0.3900 - val_accuracy: 0.8333\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3686 - accuracy: 0.8302 - val_loss: 0.4045 - val_accuracy: 0.7500\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3070 - accuracy: 0.8426 - val_loss: 0.3900 - val_accuracy: 0.8056\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3449 - accuracy: 0.8426 - val_loss: 0.3913 - val_accuracy: 0.8056\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3298 - accuracy: 0.8457 - val_loss: 0.3978 - val_accuracy: 0.8056\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3256 - accuracy: 0.8457 - val_loss: 0.3807 - val_accuracy: 0.8056\n",
      "Epoch 115/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3499 - accuracy: 0.8241 - val_loss: 0.3949 - val_accuracy: 0.7778\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3177 - accuracy: 0.8735 - val_loss: 0.4074 - val_accuracy: 0.7500\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3214 - accuracy: 0.8457 - val_loss: 0.3955 - val_accuracy: 0.7778\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3467 - accuracy: 0.8426 - val_loss: 0.3860 - val_accuracy: 0.8056\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3415 - accuracy: 0.8395 - val_loss: 0.4082 - val_accuracy: 0.7778\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3804 - accuracy: 0.8148 - val_loss: 0.3897 - val_accuracy: 0.8056\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3446 - accuracy: 0.8333 - val_loss: 0.3910 - val_accuracy: 0.7778\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3880 - accuracy: 0.8025 - val_loss: 0.4258 - val_accuracy: 0.7500\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3353 - accuracy: 0.8179 - val_loss: 0.3861 - val_accuracy: 0.8056\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3581 - accuracy: 0.8426 - val_loss: 0.3975 - val_accuracy: 0.8056\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3405 - accuracy: 0.8395 - val_loss: 0.3839 - val_accuracy: 0.8333\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3190 - accuracy: 0.8302 - val_loss: 0.4118 - val_accuracy: 0.8056\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3393 - accuracy: 0.8488 - val_loss: 0.3855 - val_accuracy: 0.8333\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3635 - accuracy: 0.8272 - val_loss: 0.4247 - val_accuracy: 0.7222\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3477 - accuracy: 0.8241 - val_loss: 0.3930 - val_accuracy: 0.8056\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3545 - accuracy: 0.8210 - val_loss: 0.4265 - val_accuracy: 0.7778\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3227 - accuracy: 0.8364 - val_loss: 0.3964 - val_accuracy: 0.8056\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3151 - accuracy: 0.8580 - val_loss: 0.4284 - val_accuracy: 0.7500\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3675 - accuracy: 0.8056 - val_loss: 0.3858 - val_accuracy: 0.8333\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3297 - accuracy: 0.8395 - val_loss: 0.3829 - val_accuracy: 0.8333\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3134 - accuracy: 0.8488 - val_loss: 0.3866 - val_accuracy: 0.8056\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3294 - accuracy: 0.8457 - val_loss: 0.3819 - val_accuracy: 0.8333\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3204 - accuracy: 0.8549 - val_loss: 0.3888 - val_accuracy: 0.8056\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3137 - accuracy: 0.8426 - val_loss: 0.4186 - val_accuracy: 0.7500\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3514 - accuracy: 0.8426 - val_loss: 0.3750 - val_accuracy: 0.8056\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3419 - accuracy: 0.8241 - val_loss: 0.3841 - val_accuracy: 0.7500\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3109 - accuracy: 0.8426 - val_loss: 0.3865 - val_accuracy: 0.7778\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3130 - accuracy: 0.8519 - val_loss: 0.3912 - val_accuracy: 0.8056\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3356 - accuracy: 0.8210 - val_loss: 0.3886 - val_accuracy: 0.8333\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3169 - accuracy: 0.8488 - val_loss: 0.4017 - val_accuracy: 0.8056\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3019 - accuracy: 0.8302 - val_loss: 0.3858 - val_accuracy: 0.8056\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3446 - accuracy: 0.8210 - val_loss: 0.3917 - val_accuracy: 0.8056\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3007 - accuracy: 0.8580 - val_loss: 0.3834 - val_accuracy: 0.8056\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3108 - accuracy: 0.8642 - val_loss: 0.3842 - val_accuracy: 0.8056\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3062 - accuracy: 0.8611 - val_loss: 0.4047 - val_accuracy: 0.7222\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3065 - accuracy: 0.8364 - val_loss: 0.3856 - val_accuracy: 0.8056\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2954 - accuracy: 0.8488 - val_loss: 0.3870 - val_accuracy: 0.8333\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2940 - accuracy: 0.8549 - val_loss: 0.3780 - val_accuracy: 0.8056\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2982 - accuracy: 0.8827 - val_loss: 0.3940 - val_accuracy: 0.7778\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2779 - accuracy: 0.8827 - val_loss: 0.3938 - val_accuracy: 0.7500\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3051 - accuracy: 0.8549 - val_loss: 0.4060 - val_accuracy: 0.7500\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3290 - accuracy: 0.8488 - val_loss: 0.3888 - val_accuracy: 0.7500\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3188 - accuracy: 0.8519 - val_loss: 0.3836 - val_accuracy: 0.8333\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.4403 - accuracy: 0.8179 - val_loss: 0.7355 - val_accuracy: 0.6667\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3754 - accuracy: 0.8210 - val_loss: 0.4350 - val_accuracy: 0.7778\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3292 - accuracy: 0.8426 - val_loss: 0.4560 - val_accuracy: 0.7222\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3692 - accuracy: 0.8056 - val_loss: 0.4030 - val_accuracy: 0.7778\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2773 - accuracy: 0.8673 - val_loss: 0.4285 - val_accuracy: 0.7500\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2894 - accuracy: 0.8735 - val_loss: 0.4064 - val_accuracy: 0.8056\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2713 - accuracy: 0.8611 - val_loss: 0.3943 - val_accuracy: 0.8056\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2873 - accuracy: 0.8642 - val_loss: 0.4151 - val_accuracy: 0.7500\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3218 - accuracy: 0.8364 - val_loss: 0.4074 - val_accuracy: 0.7222\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3050 - accuracy: 0.8488 - val_loss: 0.3993 - val_accuracy: 0.7500\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3168 - accuracy: 0.8364 - val_loss: 0.3995 - val_accuracy: 0.8056\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3117 - accuracy: 0.8704 - val_loss: 0.4463 - val_accuracy: 0.7500\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3018 - accuracy: 0.8642 - val_loss: 0.4138 - val_accuracy: 0.8056\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2774 - accuracy: 0.8611 - val_loss: 0.3942 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2888 - accuracy: 0.8735 - val_loss: 0.4125 - val_accuracy: 0.7778\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2641 - accuracy: 0.8951 - val_loss: 0.3852 - val_accuracy: 0.8333\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2910 - accuracy: 0.8519 - val_loss: 0.4217 - val_accuracy: 0.7500\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2885 - accuracy: 0.8457 - val_loss: 0.3862 - val_accuracy: 0.8333\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3198 - accuracy: 0.8086 - val_loss: 0.4049 - val_accuracy: 0.7778\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3242 - accuracy: 0.8333 - val_loss: 0.3961 - val_accuracy: 0.7500\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3278 - accuracy: 0.8179 - val_loss: 0.4298 - val_accuracy: 0.7222\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3614 - accuracy: 0.8179 - val_loss: 0.3934 - val_accuracy: 0.8056\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2989 - accuracy: 0.8457 - val_loss: 0.4186 - val_accuracy: 0.7500\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3147 - accuracy: 0.8457 - val_loss: 0.3819 - val_accuracy: 0.8333\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3025 - accuracy: 0.8580 - val_loss: 0.4179 - val_accuracy: 0.7500\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3220 - accuracy: 0.8395 - val_loss: 0.3876 - val_accuracy: 0.8056\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2797 - accuracy: 0.8642 - val_loss: 0.3995 - val_accuracy: 0.7778\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2945 - accuracy: 0.8580 - val_loss: 0.3808 - val_accuracy: 0.7778\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2562 - accuracy: 0.8735 - val_loss: 0.4027 - val_accuracy: 0.7778\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2782 - accuracy: 0.8611 - val_loss: 0.3897 - val_accuracy: 0.8056\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2718 - accuracy: 0.8642 - val_loss: 0.3992 - val_accuracy: 0.7778\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2768 - accuracy: 0.8796 - val_loss: 0.3589 - val_accuracy: 0.8056\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3167 - accuracy: 0.8519 - val_loss: 0.4794 - val_accuracy: 0.7778\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2870 - accuracy: 0.8457 - val_loss: 0.3746 - val_accuracy: 0.8056\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2662 - accuracy: 0.8611 - val_loss: 0.3786 - val_accuracy: 0.7500\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3191 - accuracy: 0.8457 - val_loss: 0.3653 - val_accuracy: 0.8056\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2804 - accuracy: 0.8735 - val_loss: 0.3753 - val_accuracy: 0.8056\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2918 - accuracy: 0.8580 - val_loss: 0.3733 - val_accuracy: 0.8333\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2378 - accuracy: 0.8858 - val_loss: 0.3990 - val_accuracy: 0.7778\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3053 - accuracy: 0.8457 - val_loss: 0.3647 - val_accuracy: 0.8333\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2857 - accuracy: 0.8549 - val_loss: 0.3962 - val_accuracy: 0.8056\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2737 - accuracy: 0.8796 - val_loss: 0.3782 - val_accuracy: 0.8056\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2779 - accuracy: 0.8673 - val_loss: 0.3737 - val_accuracy: 0.8333\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2429 - accuracy: 0.8951 - val_loss: 0.4102 - val_accuracy: 0.7778\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2879 - accuracy: 0.8426 - val_loss: 0.3675 - val_accuracy: 0.8333\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3200 - accuracy: 0.8426 - val_loss: 0.4198 - val_accuracy: 0.8056\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2746 - accuracy: 0.8611 - val_loss: 0.3928 - val_accuracy: 0.8333\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2634 - accuracy: 0.8611 - val_loss: 0.3617 - val_accuracy: 0.8333\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2671 - accuracy: 0.8673 - val_loss: 0.3652 - val_accuracy: 0.8333\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2737 - accuracy: 0.8673 - val_loss: 0.3681 - val_accuracy: 0.8333\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2820 - accuracy: 0.8858 - val_loss: 0.4386 - val_accuracy: 0.7778\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3002 - accuracy: 0.8642 - val_loss: 0.4009 - val_accuracy: 0.7500\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2725 - accuracy: 0.8642 - val_loss: 0.4029 - val_accuracy: 0.7778\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2532 - accuracy: 0.8796 - val_loss: 0.3972 - val_accuracy: 0.7778\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2421 - accuracy: 0.8796 - val_loss: 0.3945 - val_accuracy: 0.7778\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3183 - accuracy: 0.8333 - val_loss: 0.4085 - val_accuracy: 0.7778\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2757 - accuracy: 0.8426 - val_loss: 0.3603 - val_accuracy: 0.8333\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2707 - accuracy: 0.8519 - val_loss: 0.3862 - val_accuracy: 0.7500\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2757 - accuracy: 0.8395 - val_loss: 0.3841 - val_accuracy: 0.8056\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2319 - accuracy: 0.8951 - val_loss: 0.3882 - val_accuracy: 0.8056\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2319 - accuracy: 0.8827 - val_loss: 0.3942 - val_accuracy: 0.7778\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3102 - accuracy: 0.8488 - val_loss: 0.3933 - val_accuracy: 0.7778\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2740 - accuracy: 0.8642 - val_loss: 0.3869 - val_accuracy: 0.8333\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2797 - accuracy: 0.8704 - val_loss: 0.4094 - val_accuracy: 0.7500\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2607 - accuracy: 0.8673 - val_loss: 0.3969 - val_accuracy: 0.8056\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2549 - accuracy: 0.8765 - val_loss: 0.4053 - val_accuracy: 0.8056\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2710 - accuracy: 0.8642 - val_loss: 0.4002 - val_accuracy: 0.8056\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2752 - accuracy: 0.8580 - val_loss: 0.4138 - val_accuracy: 0.7500\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2643 - accuracy: 0.8549 - val_loss: 0.3899 - val_accuracy: 0.7500\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2328 - accuracy: 0.8827 - val_loss: 0.3976 - val_accuracy: 0.7778\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2796 - accuracy: 0.8611 - val_loss: 0.3828 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.2525 - accuracy: 0.8580 - val_loss: 0.4091 - val_accuracy: 0.7500\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3235 - accuracy: 0.8333 - val_loss: 0.3673 - val_accuracy: 0.8056\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3327 - accuracy: 0.8333 - val_loss: 0.5114 - val_accuracy: 0.7778\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3022 - accuracy: 0.8519 - val_loss: 0.3666 - val_accuracy: 0.8333\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2602 - accuracy: 0.8796 - val_loss: 0.4164 - val_accuracy: 0.7500\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2452 - accuracy: 0.8796 - val_loss: 0.4078 - val_accuracy: 0.7500\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2461 - accuracy: 0.8673 - val_loss: 0.4242 - val_accuracy: 0.7500\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2497 - accuracy: 0.8611 - val_loss: 0.4017 - val_accuracy: 0.7500\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3004 - accuracy: 0.8519 - val_loss: 0.3856 - val_accuracy: 0.7500\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2513 - accuracy: 0.8580 - val_loss: 0.4011 - val_accuracy: 0.7500\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2663 - accuracy: 0.8765 - val_loss: 0.4040 - val_accuracy: 0.7500\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2605 - accuracy: 0.8858 - val_loss: 0.3850 - val_accuracy: 0.7500\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2322 - accuracy: 0.8920 - val_loss: 0.3795 - val_accuracy: 0.8056\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2539 - accuracy: 0.8765 - val_loss: 0.4511 - val_accuracy: 0.7778\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2643 - accuracy: 0.8827 - val_loss: 0.3779 - val_accuracy: 0.8056\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2426 - accuracy: 0.8735 - val_loss: 0.4032 - val_accuracy: 0.7778\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2861 - accuracy: 0.8673 - val_loss: 0.4172 - val_accuracy: 0.7500\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2623 - accuracy: 0.8426 - val_loss: 0.4033 - val_accuracy: 0.7222\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2340 - accuracy: 0.8858 - val_loss: 0.4240 - val_accuracy: 0.7500\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2737 - accuracy: 0.8426 - val_loss: 0.3840 - val_accuracy: 0.8056\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2213 - accuracy: 0.9012 - val_loss: 0.3854 - val_accuracy: 0.7778\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2528 - accuracy: 0.8673 - val_loss: 0.3934 - val_accuracy: 0.8333\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2595 - accuracy: 0.8519 - val_loss: 0.4129 - val_accuracy: 0.8056\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2413 - accuracy: 0.8704 - val_loss: 0.3722 - val_accuracy: 0.8056\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2559 - accuracy: 0.8735 - val_loss: 0.3506 - val_accuracy: 0.7778\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2522 - accuracy: 0.8735 - val_loss: 0.3583 - val_accuracy: 0.7778\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2420 - accuracy: 0.8889 - val_loss: 0.3497 - val_accuracy: 0.8333\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2889 - accuracy: 0.8673 - val_loss: 0.4909 - val_accuracy: 0.7500\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2981 - accuracy: 0.8457 - val_loss: 0.3581 - val_accuracy: 0.8056\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2626 - accuracy: 0.8642 - val_loss: 0.3944 - val_accuracy: 0.8056\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2625 - accuracy: 0.8765 - val_loss: 0.3651 - val_accuracy: 0.7778\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2630 - accuracy: 0.8673 - val_loss: 0.4065 - val_accuracy: 0.6944\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3035 - accuracy: 0.8765 - val_loss: 0.3549 - val_accuracy: 0.7778\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2380 - accuracy: 0.8827 - val_loss: 0.3442 - val_accuracy: 0.8333\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2735 - accuracy: 0.8549 - val_loss: 0.3665 - val_accuracy: 0.8333\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2462 - accuracy: 0.8827 - val_loss: 0.3675 - val_accuracy: 0.8333\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2398 - accuracy: 0.8796 - val_loss: 0.3899 - val_accuracy: 0.7778\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2291 - accuracy: 0.8889 - val_loss: 0.3871 - val_accuracy: 0.7778\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2437 - accuracy: 0.8827 - val_loss: 0.3736 - val_accuracy: 0.8056\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2422 - accuracy: 0.8796 - val_loss: 0.3756 - val_accuracy: 0.7778\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2353 - accuracy: 0.8858 - val_loss: 0.4282 - val_accuracy: 0.7500\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2497 - accuracy: 0.8889 - val_loss: 0.3677 - val_accuracy: 0.8056\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2676 - accuracy: 0.8611 - val_loss: 0.3818 - val_accuracy: 0.8333\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2455 - accuracy: 0.8858 - val_loss: 0.4086 - val_accuracy: 0.7500\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2615 - accuracy: 0.8673 - val_loss: 0.3578 - val_accuracy: 0.7778\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2401 - accuracy: 0.8981 - val_loss: 0.3632 - val_accuracy: 0.7500\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2323 - accuracy: 0.8920 - val_loss: 0.3833 - val_accuracy: 0.7778\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2178 - accuracy: 0.8920 - val_loss: 0.3768 - val_accuracy: 0.7222\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2284 - accuracy: 0.8796 - val_loss: 0.3650 - val_accuracy: 0.7778\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2446 - accuracy: 0.8735 - val_loss: 0.3742 - val_accuracy: 0.8056\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2340 - accuracy: 0.8735 - val_loss: 0.3744 - val_accuracy: 0.7500\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2377 - accuracy: 0.8611 - val_loss: 0.4103 - val_accuracy: 0.7500\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2235 - accuracy: 0.8796 - val_loss: 0.3792 - val_accuracy: 0.7778\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2135 - accuracy: 0.8827 - val_loss: 0.4298 - val_accuracy: 0.7500\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2197 - accuracy: 0.8796 - val_loss: 0.3813 - val_accuracy: 0.7778\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2250 - accuracy: 0.8951 - val_loss: 0.4590 - val_accuracy: 0.7500\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2538 - accuracy: 0.8796 - val_loss: 0.3736 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2115 - accuracy: 0.8889 - val_loss: 0.4127 - val_accuracy: 0.8056\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2044 - accuracy: 0.9105 - val_loss: 0.4126 - val_accuracy: 0.7500\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2493 - accuracy: 0.8704 - val_loss: 0.3656 - val_accuracy: 0.8056\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2853 - accuracy: 0.8519 - val_loss: 0.3610 - val_accuracy: 0.8056\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2336 - accuracy: 0.8796 - val_loss: 0.3966 - val_accuracy: 0.7500\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2268 - accuracy: 0.8889 - val_loss: 0.3743 - val_accuracy: 0.8056\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2189 - accuracy: 0.8981 - val_loss: 0.3638 - val_accuracy: 0.8056\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2261 - accuracy: 0.9012 - val_loss: 0.4085 - val_accuracy: 0.7500\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2064 - accuracy: 0.9043 - val_loss: 0.4027 - val_accuracy: 0.8056\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2237 - accuracy: 0.8951 - val_loss: 0.3669 - val_accuracy: 0.7778\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2476 - accuracy: 0.8981 - val_loss: 0.3707 - val_accuracy: 0.8056\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2747 - accuracy: 0.8642 - val_loss: 0.6019 - val_accuracy: 0.7222\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4069 - accuracy: 0.8210 - val_loss: 0.4125 - val_accuracy: 0.7500\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2976 - accuracy: 0.8765 - val_loss: 0.4076 - val_accuracy: 0.7778\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2384 - accuracy: 0.8611 - val_loss: 0.3686 - val_accuracy: 0.8056\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2407 - accuracy: 0.8735 - val_loss: 0.3663 - val_accuracy: 0.7778\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2456 - accuracy: 0.8765 - val_loss: 0.3494 - val_accuracy: 0.8056\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2426 - accuracy: 0.8981 - val_loss: 0.3979 - val_accuracy: 0.7778\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2090 - accuracy: 0.9136 - val_loss: 0.4369 - val_accuracy: 0.7500\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2337 - accuracy: 0.8796 - val_loss: 0.3626 - val_accuracy: 0.8056\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.2435 - accuracy: 0.8827 - val_loss: 0.4297 - val_accuracy: 0.7500\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2130 - accuracy: 0.8951 - val_loss: 0.3907 - val_accuracy: 0.7778\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2496 - accuracy: 0.8673 - val_loss: 0.3760 - val_accuracy: 0.8056\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1741 - accuracy: 0.9228 - val_loss: 0.3926 - val_accuracy: 0.7222\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2197 - accuracy: 0.9012 - val_loss: 0.3860 - val_accuracy: 0.8333\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2152 - accuracy: 0.9012 - val_loss: 0.4075 - val_accuracy: 0.7778\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2085 - accuracy: 0.9105 - val_loss: 0.4496 - val_accuracy: 0.7500\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2877 - accuracy: 0.8735 - val_loss: 0.3974 - val_accuracy: 0.7778\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.3908 - accuracy: 0.8210 - val_loss: 0.6680 - val_accuracy: 0.7222\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2904 - accuracy: 0.8549 - val_loss: 0.3890 - val_accuracy: 0.8056\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2220 - accuracy: 0.9074 - val_loss: 0.4866 - val_accuracy: 0.7500\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2500 - accuracy: 0.8673 - val_loss: 0.3840 - val_accuracy: 0.7778\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.2867 - accuracy: 0.8549 - val_loss: 0.3775 - val_accuracy: 0.8056\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2640 - accuracy: 0.8765 - val_loss: 0.3863 - val_accuracy: 0.7778\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2430 - accuracy: 0.8765 - val_loss: 0.3625 - val_accuracy: 0.8056\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2090 - accuracy: 0.8981 - val_loss: 0.4033 - val_accuracy: 0.7778\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2235 - accuracy: 0.9012 - val_loss: 0.3576 - val_accuracy: 0.7778\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2130 - accuracy: 0.9012 - val_loss: 0.3802 - val_accuracy: 0.7778\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2168 - accuracy: 0.9012 - val_loss: 0.3781 - val_accuracy: 0.8056\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2210 - accuracy: 0.8920 - val_loss: 0.3661 - val_accuracy: 0.8056\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2273 - accuracy: 0.8827 - val_loss: 0.4013 - val_accuracy: 0.7778\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2416 - accuracy: 0.8827 - val_loss: 0.3852 - val_accuracy: 0.7778\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2348 - accuracy: 0.8951 - val_loss: 0.3656 - val_accuracy: 0.8333\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2365 - accuracy: 0.8920 - val_loss: 0.3575 - val_accuracy: 0.8056\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2095 - accuracy: 0.9012 - val_loss: 0.3803 - val_accuracy: 0.8056\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2125 - accuracy: 0.8951 - val_loss: 0.3641 - val_accuracy: 0.7778\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2132 - accuracy: 0.8920 - val_loss: 0.3660 - val_accuracy: 0.8333\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1971 - accuracy: 0.8981 - val_loss: 0.3569 - val_accuracy: 0.7500\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1868 - accuracy: 0.9198 - val_loss: 0.3937 - val_accuracy: 0.7500\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2088 - accuracy: 0.9012 - val_loss: 0.3587 - val_accuracy: 0.8333\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2278 - accuracy: 0.8951 - val_loss: 0.3708 - val_accuracy: 0.7778\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2212 - accuracy: 0.8920 - val_loss: 0.3721 - val_accuracy: 0.8056\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2000 - accuracy: 0.9074 - val_loss: 0.3643 - val_accuracy: 0.8056\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1989 - accuracy: 0.9198 - val_loss: 0.3742 - val_accuracy: 0.8056\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2116 - accuracy: 0.8858 - val_loss: 0.3670 - val_accuracy: 0.7778\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2148 - accuracy: 0.8858 - val_loss: 0.3835 - val_accuracy: 0.8333\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2238 - accuracy: 0.8858 - val_loss: 0.3754 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2301 - accuracy: 0.8920 - val_loss: 0.4282 - val_accuracy: 0.7778\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2161 - accuracy: 0.8889 - val_loss: 0.4337 - val_accuracy: 0.7222\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2310 - accuracy: 0.8858 - val_loss: 0.3917 - val_accuracy: 0.7500\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1985 - accuracy: 0.9198 - val_loss: 0.4654 - val_accuracy: 0.7500\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2056 - accuracy: 0.8981 - val_loss: 0.3741 - val_accuracy: 0.8056\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2041 - accuracy: 0.9136 - val_loss: 0.4448 - val_accuracy: 0.7500\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1986 - accuracy: 0.9074 - val_loss: 0.3789 - val_accuracy: 0.8056\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2301 - accuracy: 0.8920 - val_loss: 0.4245 - val_accuracy: 0.7778\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2329 - accuracy: 0.8827 - val_loss: 0.4846 - val_accuracy: 0.6944\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2054 - accuracy: 0.9043 - val_loss: 0.3948 - val_accuracy: 0.8056\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2276 - accuracy: 0.8765 - val_loss: 0.4470 - val_accuracy: 0.7500\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2097 - accuracy: 0.8827 - val_loss: 0.3822 - val_accuracy: 0.8056\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1936 - accuracy: 0.9105 - val_loss: 0.4188 - val_accuracy: 0.8056\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2113 - accuracy: 0.8889 - val_loss: 0.3778 - val_accuracy: 0.8056\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2004 - accuracy: 0.9136 - val_loss: 0.4219 - val_accuracy: 0.7222\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2453 - accuracy: 0.8704 - val_loss: 0.4222 - val_accuracy: 0.7222\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2027 - accuracy: 0.9012 - val_loss: 0.4060 - val_accuracy: 0.7778\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1610 - accuracy: 0.9290 - val_loss: 0.3931 - val_accuracy: 0.7778\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2208 - accuracy: 0.9074 - val_loss: 0.4229 - val_accuracy: 0.7500\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2785 - accuracy: 0.8858 - val_loss: 0.4149 - val_accuracy: 0.7222\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2186 - accuracy: 0.9074 - val_loss: 0.3741 - val_accuracy: 0.8056\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1818 - accuracy: 0.9167 - val_loss: 0.4283 - val_accuracy: 0.8056\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2203 - accuracy: 0.8889 - val_loss: 0.3944 - val_accuracy: 0.8056\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1858 - accuracy: 0.9198 - val_loss: 0.4173 - val_accuracy: 0.7222\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2266 - accuracy: 0.8920 - val_loss: 0.4032 - val_accuracy: 0.7778\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1727 - accuracy: 0.9228 - val_loss: 0.3994 - val_accuracy: 0.7778\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1551 - accuracy: 0.9198 - val_loss: 0.4471 - val_accuracy: 0.7222\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2056 - accuracy: 0.9105 - val_loss: 0.3982 - val_accuracy: 0.7778\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1915 - accuracy: 0.9105 - val_loss: 0.4169 - val_accuracy: 0.8056\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1968 - accuracy: 0.9043 - val_loss: 0.5415 - val_accuracy: 0.7222\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2276 - accuracy: 0.8796 - val_loss: 0.4073 - val_accuracy: 0.8056\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2086 - accuracy: 0.9012 - val_loss: 0.4697 - val_accuracy: 0.7222\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1855 - accuracy: 0.9259 - val_loss: 0.4122 - val_accuracy: 0.7500\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1779 - accuracy: 0.9074 - val_loss: 0.4499 - val_accuracy: 0.7778\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2433 - accuracy: 0.8920 - val_loss: 0.3964 - val_accuracy: 0.7500\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1877 - accuracy: 0.9043 - val_loss: 0.4360 - val_accuracy: 0.6944\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1963 - accuracy: 0.9105 - val_loss: 0.3988 - val_accuracy: 0.7222\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1927 - accuracy: 0.9105 - val_loss: 0.4036 - val_accuracy: 0.6944\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2139 - accuracy: 0.8858 - val_loss: 0.3921 - val_accuracy: 0.7500\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1883 - accuracy: 0.9167 - val_loss: 0.4025 - val_accuracy: 0.8333\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2085 - accuracy: 0.9043 - val_loss: 0.3790 - val_accuracy: 0.7778\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2627 - accuracy: 0.8920 - val_loss: 0.3657 - val_accuracy: 0.7778\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2042 - accuracy: 0.9105 - val_loss: 0.3858 - val_accuracy: 0.7222\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1835 - accuracy: 0.9228 - val_loss: 0.3801 - val_accuracy: 0.7778\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1670 - accuracy: 0.9414 - val_loss: 0.3626 - val_accuracy: 0.7778\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2016 - accuracy: 0.9043 - val_loss: 0.4002 - val_accuracy: 0.7500\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1859 - accuracy: 0.9167 - val_loss: 0.4170 - val_accuracy: 0.6944\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2069 - accuracy: 0.9259 - val_loss: 0.5103 - val_accuracy: 0.7500\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2071 - accuracy: 0.9043 - val_loss: 0.3992 - val_accuracy: 0.7500\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1852 - accuracy: 0.9105 - val_loss: 0.4233 - val_accuracy: 0.7500\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2118 - accuracy: 0.9198 - val_loss: 0.4107 - val_accuracy: 0.7778\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1941 - accuracy: 0.9074 - val_loss: 0.4160 - val_accuracy: 0.7500\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1554 - accuracy: 0.9198 - val_loss: 0.4219 - val_accuracy: 0.7778\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1585 - accuracy: 0.9198 - val_loss: 0.4169 - val_accuracy: 0.7500\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1827 - accuracy: 0.9290 - val_loss: 0.4438 - val_accuracy: 0.7500\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1740 - accuracy: 0.9167 - val_loss: 0.4160 - val_accuracy: 0.8056\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2106 - accuracy: 0.8981 - val_loss: 0.4238 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2281 - accuracy: 0.9043 - val_loss: 0.4557 - val_accuracy: 0.7222\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1831 - accuracy: 0.9290 - val_loss: 0.4394 - val_accuracy: 0.7500\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2352 - accuracy: 0.9105 - val_loss: 0.4161 - val_accuracy: 0.7778\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1811 - accuracy: 0.9136 - val_loss: 0.4494 - val_accuracy: 0.6944\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1847 - accuracy: 0.9167 - val_loss: 0.3933 - val_accuracy: 0.7778\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1939 - accuracy: 0.9228 - val_loss: 0.4128 - val_accuracy: 0.7778\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1899 - accuracy: 0.9105 - val_loss: 0.4693 - val_accuracy: 0.7778\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1840 - accuracy: 0.9167 - val_loss: 0.4298 - val_accuracy: 0.7500\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9475 - val_loss: 0.4941 - val_accuracy: 0.7500\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.2046 - accuracy: 0.9074 - val_loss: 0.4277 - val_accuracy: 0.7222\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2442 - accuracy: 0.8735 - val_loss: 0.6270 - val_accuracy: 0.6944\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1970 - accuracy: 0.9043 - val_loss: 0.4138 - val_accuracy: 0.7778\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1548 - accuracy: 0.9259 - val_loss: 0.4598 - val_accuracy: 0.7500\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1840 - accuracy: 0.9228 - val_loss: 0.4410 - val_accuracy: 0.8056\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1668 - accuracy: 0.9136 - val_loss: 0.4108 - val_accuracy: 0.7778\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1986 - accuracy: 0.9136 - val_loss: 0.5174 - val_accuracy: 0.6944\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1874 - accuracy: 0.9228 - val_loss: 0.4328 - val_accuracy: 0.6944\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2164 - accuracy: 0.9012 - val_loss: 0.5150 - val_accuracy: 0.6944\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1646 - accuracy: 0.9228 - val_loss: 0.4531 - val_accuracy: 0.8056\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1626 - accuracy: 0.9259 - val_loss: 0.4369 - val_accuracy: 0.7778\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1471 - accuracy: 0.9568 - val_loss: 0.4150 - val_accuracy: 0.7778\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1541 - accuracy: 0.9321 - val_loss: 0.4348 - val_accuracy: 0.7500\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1640 - accuracy: 0.9259 - val_loss: 0.4472 - val_accuracy: 0.7778\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1740 - accuracy: 0.9290 - val_loss: 0.4303 - val_accuracy: 0.7778\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1579 - accuracy: 0.9475 - val_loss: 0.4178 - val_accuracy: 0.7778\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1952 - accuracy: 0.9105 - val_loss: 0.4307 - val_accuracy: 0.7778\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1636 - accuracy: 0.9198 - val_loss: 0.4619 - val_accuracy: 0.7222\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1715 - accuracy: 0.9290 - val_loss: 0.4515 - val_accuracy: 0.7500\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1507 - accuracy: 0.9290 - val_loss: 0.4438 - val_accuracy: 0.7500\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1604 - accuracy: 0.9321 - val_loss: 0.4636 - val_accuracy: 0.7222\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1772 - accuracy: 0.9136 - val_loss: 0.4135 - val_accuracy: 0.7778\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1607 - accuracy: 0.9352 - val_loss: 0.4041 - val_accuracy: 0.8056\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1590 - accuracy: 0.9414 - val_loss: 0.4200 - val_accuracy: 0.7778\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3068 - accuracy: 0.8704 - val_loss: 1.0280 - val_accuracy: 0.6667\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.3336 - accuracy: 0.8704 - val_loss: 0.4447 - val_accuracy: 0.7500\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2374 - accuracy: 0.9105 - val_loss: 0.5317 - val_accuracy: 0.7222\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1842 - accuracy: 0.9290 - val_loss: 0.4486 - val_accuracy: 0.7500\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2044 - accuracy: 0.8827 - val_loss: 0.4310 - val_accuracy: 0.7778\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1515 - accuracy: 0.9290 - val_loss: 0.5132 - val_accuracy: 0.7778\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1451 - accuracy: 0.9444 - val_loss: 0.4262 - val_accuracy: 0.7778\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1919 - accuracy: 0.9167 - val_loss: 0.4470 - val_accuracy: 0.7778\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1734 - accuracy: 0.9290 - val_loss: 0.4990 - val_accuracy: 0.7778\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1804 - accuracy: 0.9167 - val_loss: 0.4595 - val_accuracy: 0.7222\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2399 - accuracy: 0.8951 - val_loss: 0.4348 - val_accuracy: 0.7778\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2057 - accuracy: 0.8981 - val_loss: 0.4461 - val_accuracy: 0.7500\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1578 - accuracy: 0.9228 - val_loss: 0.4492 - val_accuracy: 0.7778\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1853 - accuracy: 0.9228 - val_loss: 0.4727 - val_accuracy: 0.7500\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1423 - accuracy: 0.9475 - val_loss: 0.4404 - val_accuracy: 0.7500\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1773 - accuracy: 0.9321 - val_loss: 0.4141 - val_accuracy: 0.7778\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1545 - accuracy: 0.9228 - val_loss: 0.4638 - val_accuracy: 0.7778\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1421 - accuracy: 0.9383 - val_loss: 0.4723 - val_accuracy: 0.7778\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1440 - accuracy: 0.9321 - val_loss: 0.4610 - val_accuracy: 0.7778\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1633 - accuracy: 0.9228 - val_loss: 0.4968 - val_accuracy: 0.7222\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1985 - accuracy: 0.9043 - val_loss: 0.4301 - val_accuracy: 0.8056\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1839 - accuracy: 0.9074 - val_loss: 0.4420 - val_accuracy: 0.8056\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9444 - val_loss: 0.4284 - val_accuracy: 0.7500\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1657 - accuracy: 0.9228 - val_loss: 0.4425 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1742 - accuracy: 0.9198 - val_loss: 0.4447 - val_accuracy: 0.8333\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1284 - accuracy: 0.9352 - val_loss: 0.4400 - val_accuracy: 0.7778\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1741 - accuracy: 0.9105 - val_loss: 0.4663 - val_accuracy: 0.7778\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1617 - accuracy: 0.9321 - val_loss: 0.4281 - val_accuracy: 0.7778\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1527 - accuracy: 0.9321 - val_loss: 0.4789 - val_accuracy: 0.7778\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1450 - accuracy: 0.9352 - val_loss: 0.4328 - val_accuracy: 0.7778\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1593 - accuracy: 0.9259 - val_loss: 0.4291 - val_accuracy: 0.7500\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1630 - accuracy: 0.9043 - val_loss: 0.4077 - val_accuracy: 0.7778\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1727 - accuracy: 0.9414 - val_loss: 0.4120 - val_accuracy: 0.8056\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1891 - accuracy: 0.9105 - val_loss: 0.4043 - val_accuracy: 0.7778\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1616 - accuracy: 0.9290 - val_loss: 0.4350 - val_accuracy: 0.8056\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1502 - accuracy: 0.9321 - val_loss: 0.4304 - val_accuracy: 0.7500\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1675 - accuracy: 0.9290 - val_loss: 0.4043 - val_accuracy: 0.8056\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1667 - accuracy: 0.9290 - val_loss: 0.4287 - val_accuracy: 0.8056\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1761 - accuracy: 0.9043 - val_loss: 0.4519 - val_accuracy: 0.7778\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1644 - accuracy: 0.9352 - val_loss: 0.4571 - val_accuracy: 0.7778\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1746 - accuracy: 0.9228 - val_loss: 0.4771 - val_accuracy: 0.7500\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1558 - accuracy: 0.9352 - val_loss: 0.5189 - val_accuracy: 0.7500\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1501 - accuracy: 0.9321 - val_loss: 0.4012 - val_accuracy: 0.7778\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1959 - accuracy: 0.9074 - val_loss: 0.4986 - val_accuracy: 0.7500\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1785 - accuracy: 0.9321 - val_loss: 0.4355 - val_accuracy: 0.7778\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1693 - accuracy: 0.9259 - val_loss: 0.4575 - val_accuracy: 0.8056\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1885 - accuracy: 0.9167 - val_loss: 0.4383 - val_accuracy: 0.8056\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1709 - accuracy: 0.9259 - val_loss: 0.4377 - val_accuracy: 0.7778\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1395 - accuracy: 0.9475 - val_loss: 0.4376 - val_accuracy: 0.8056\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9568 - val_loss: 0.4987 - val_accuracy: 0.7222\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1754 - accuracy: 0.9198 - val_loss: 0.4693 - val_accuracy: 0.7500\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1650 - accuracy: 0.9228 - val_loss: 0.4985 - val_accuracy: 0.7778\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1783 - accuracy: 0.9074 - val_loss: 0.4663 - val_accuracy: 0.7778\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1602 - accuracy: 0.9506 - val_loss: 0.5117 - val_accuracy: 0.7222\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1769 - accuracy: 0.9228 - val_loss: 0.4739 - val_accuracy: 0.7778\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1518 - accuracy: 0.9290 - val_loss: 0.4729 - val_accuracy: 0.7500\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1404 - accuracy: 0.9321 - val_loss: 0.4649 - val_accuracy: 0.7778\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1697 - accuracy: 0.9259 - val_loss: 0.4717 - val_accuracy: 0.7778\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1186 - accuracy: 0.9506 - val_loss: 0.4620 - val_accuracy: 0.6944\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1525 - accuracy: 0.9352 - val_loss: 0.4450 - val_accuracy: 0.7500\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1579 - accuracy: 0.9475 - val_loss: 0.4955 - val_accuracy: 0.7222\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1524 - accuracy: 0.9444 - val_loss: 0.4566 - val_accuracy: 0.8056\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1325 - accuracy: 0.9321 - val_loss: 0.4571 - val_accuracy: 0.7778\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9383 - val_loss: 0.4499 - val_accuracy: 0.7778\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1263 - accuracy: 0.9414 - val_loss: 0.4900 - val_accuracy: 0.7222\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1171 - accuracy: 0.9537 - val_loss: 0.5103 - val_accuracy: 0.6944\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1477 - accuracy: 0.9444 - val_loss: 0.5398 - val_accuracy: 0.7222\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1474 - accuracy: 0.9259 - val_loss: 0.5506 - val_accuracy: 0.7222\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1332 - accuracy: 0.9383 - val_loss: 0.5245 - val_accuracy: 0.7500\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1616 - accuracy: 0.9290 - val_loss: 0.5374 - val_accuracy: 0.7500\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1698 - accuracy: 0.9136 - val_loss: 0.5753 - val_accuracy: 0.7222\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1422 - accuracy: 0.9290 - val_loss: 0.5257 - val_accuracy: 0.7778\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1183 - accuracy: 0.9568 - val_loss: 0.5165 - val_accuracy: 0.7500\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1278 - accuracy: 0.9352 - val_loss: 0.5595 - val_accuracy: 0.6944\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1169 - accuracy: 0.9599 - val_loss: 0.6286 - val_accuracy: 0.7500\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1861 - accuracy: 0.9198 - val_loss: 0.4885 - val_accuracy: 0.7778\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2052 - accuracy: 0.9043 - val_loss: 0.6188 - val_accuracy: 0.7222\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.2647 - accuracy: 0.8735 - val_loss: 0.5347 - val_accuracy: 0.8056\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1875 - accuracy: 0.9043 - val_loss: 0.4649 - val_accuracy: 0.7778\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1536 - accuracy: 0.9321 - val_loss: 0.5575 - val_accuracy: 0.7778\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1671 - accuracy: 0.9259 - val_loss: 0.4802 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1638 - accuracy: 0.9290 - val_loss: 0.4829 - val_accuracy: 0.7500\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1152 - accuracy: 0.9475 - val_loss: 0.5190 - val_accuracy: 0.7500\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1340 - accuracy: 0.9475 - val_loss: 0.5180 - val_accuracy: 0.8056\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1470 - accuracy: 0.9352 - val_loss: 0.5521 - val_accuracy: 0.7778\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1160 - accuracy: 0.9475 - val_loss: 0.5636 - val_accuracy: 0.7500\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9414 - val_loss: 0.5471 - val_accuracy: 0.7500\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9414 - val_loss: 0.5156 - val_accuracy: 0.7778\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1770 - accuracy: 0.9290 - val_loss: 0.5494 - val_accuracy: 0.7222\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1137 - accuracy: 0.9568 - val_loss: 0.5332 - val_accuracy: 0.7222\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1721 - accuracy: 0.9259 - val_loss: 0.4513 - val_accuracy: 0.8056\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1749 - accuracy: 0.9228 - val_loss: 0.4807 - val_accuracy: 0.7778\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1578 - accuracy: 0.9383 - val_loss: 0.4686 - val_accuracy: 0.7778\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1301 - accuracy: 0.9568 - val_loss: 0.4832 - val_accuracy: 0.7500\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1397 - accuracy: 0.9537 - val_loss: 0.4679 - val_accuracy: 0.7778\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1473 - accuracy: 0.9321 - val_loss: 0.4855 - val_accuracy: 0.7778\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1550 - accuracy: 0.9259 - val_loss: 0.4361 - val_accuracy: 0.7778\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1399 - accuracy: 0.9537 - val_loss: 0.4763 - val_accuracy: 0.8056\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1388 - accuracy: 0.9414 - val_loss: 0.4978 - val_accuracy: 0.8056\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1354 - accuracy: 0.9414 - val_loss: 0.4833 - val_accuracy: 0.8056\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1266 - accuracy: 0.9414 - val_loss: 0.5082 - val_accuracy: 0.8056\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1735 - accuracy: 0.9136 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1252 - accuracy: 0.9475 - val_loss: 0.5090 - val_accuracy: 0.7500\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9444 - val_loss: 0.4699 - val_accuracy: 0.8056\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9475 - val_loss: 0.4845 - val_accuracy: 0.7222\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1233 - accuracy: 0.9444 - val_loss: 0.5422 - val_accuracy: 0.7222\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1223 - accuracy: 0.9506 - val_loss: 0.5060 - val_accuracy: 0.7500\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1522 - accuracy: 0.9228 - val_loss: 0.5041 - val_accuracy: 0.7778\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1354 - accuracy: 0.9383 - val_loss: 0.5052 - val_accuracy: 0.7778\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1026 - accuracy: 0.9660 - val_loss: 0.4677 - val_accuracy: 0.8056\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9691 - val_loss: 0.5052 - val_accuracy: 0.7500\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1757 - accuracy: 0.9352 - val_loss: 0.4417 - val_accuracy: 0.7778\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1140 - accuracy: 0.9475 - val_loss: 0.4768 - val_accuracy: 0.7222\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1121 - accuracy: 0.9537 - val_loss: 0.5111 - val_accuracy: 0.7222\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1761 - accuracy: 0.9383 - val_loss: 0.4466 - val_accuracy: 0.7500\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2089 - accuracy: 0.9136 - val_loss: 0.7533 - val_accuracy: 0.7500\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.2816 - accuracy: 0.8827 - val_loss: 0.4404 - val_accuracy: 0.7778\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1609 - accuracy: 0.9259 - val_loss: 0.4064 - val_accuracy: 0.7778\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1271 - accuracy: 0.9475 - val_loss: 0.4603 - val_accuracy: 0.7500\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1381 - accuracy: 0.9259 - val_loss: 0.4969 - val_accuracy: 0.7778\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1888 - accuracy: 0.9321 - val_loss: 0.5049 - val_accuracy: 0.7222\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1553 - accuracy: 0.9352 - val_loss: 0.4812 - val_accuracy: 0.7222\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1283 - accuracy: 0.9506 - val_loss: 0.4601 - val_accuracy: 0.8333\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1175 - accuracy: 0.9599 - val_loss: 0.4631 - val_accuracy: 0.7500\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1280 - accuracy: 0.9383 - val_loss: 0.4269 - val_accuracy: 0.7778\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1120 - accuracy: 0.9568 - val_loss: 0.4615 - val_accuracy: 0.8056\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1817 - accuracy: 0.9167 - val_loss: 0.4444 - val_accuracy: 0.7778\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1327 - accuracy: 0.9352 - val_loss: 0.4420 - val_accuracy: 0.7778\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1409 - accuracy: 0.9383 - val_loss: 0.4320 - val_accuracy: 0.7778\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1277 - accuracy: 0.9599 - val_loss: 0.4509 - val_accuracy: 0.8056\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1468 - accuracy: 0.9568 - val_loss: 0.4865 - val_accuracy: 0.8056\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1264 - accuracy: 0.9352 - val_loss: 0.4652 - val_accuracy: 0.7500\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1516 - accuracy: 0.9383 - val_loss: 0.4339 - val_accuracy: 0.7778\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1575 - accuracy: 0.9383 - val_loss: 0.4601 - val_accuracy: 0.7500\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1588 - accuracy: 0.9414 - val_loss: 0.4294 - val_accuracy: 0.8056\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1322 - accuracy: 0.9537 - val_loss: 0.4795 - val_accuracy: 0.7778\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1260 - accuracy: 0.9414 - val_loss: 0.4407 - val_accuracy: 0.7778\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1127 - accuracy: 0.9444 - val_loss: 0.4128 - val_accuracy: 0.8056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1244 - accuracy: 0.9568 - val_loss: 0.4640 - val_accuracy: 0.7778\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1398 - accuracy: 0.9444 - val_loss: 0.4229 - val_accuracy: 0.7778\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1360 - accuracy: 0.9290 - val_loss: 0.4876 - val_accuracy: 0.7778\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1186 - accuracy: 0.9599 - val_loss: 0.4987 - val_accuracy: 0.8056\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1430 - accuracy: 0.9352 - val_loss: 0.5017 - val_accuracy: 0.7778\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1287 - accuracy: 0.9321 - val_loss: 0.4895 - val_accuracy: 0.7778\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1017 - accuracy: 0.9568 - val_loss: 0.5030 - val_accuracy: 0.7500\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1301 - accuracy: 0.9383 - val_loss: 0.5013 - val_accuracy: 0.7500\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1278 - accuracy: 0.9506 - val_loss: 0.4646 - val_accuracy: 0.7500\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1016 - accuracy: 0.9599 - val_loss: 0.4217 - val_accuracy: 0.7778\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1728 - accuracy: 0.9198 - val_loss: 0.4609 - val_accuracy: 0.7500\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1273 - accuracy: 0.9475 - val_loss: 0.5494 - val_accuracy: 0.7222\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1496 - accuracy: 0.9414 - val_loss: 0.4782 - val_accuracy: 0.7500\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9414 - val_loss: 0.4557 - val_accuracy: 0.7778\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1047 - accuracy: 0.9506 - val_loss: 0.4888 - val_accuracy: 0.7778\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0917 - accuracy: 0.9691 - val_loss: 0.5866 - val_accuracy: 0.6944\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9414 - val_loss: 0.5466 - val_accuracy: 0.7778\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1311 - accuracy: 0.9506 - val_loss: 0.5620 - val_accuracy: 0.7222\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1396 - accuracy: 0.9444 - val_loss: 0.5185 - val_accuracy: 0.7500\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1383 - accuracy: 0.9444 - val_loss: 0.5108 - val_accuracy: 0.7500\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1463 - accuracy: 0.9414 - val_loss: 0.4807 - val_accuracy: 0.7500\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1067 - accuracy: 0.9568 - val_loss: 0.4882 - val_accuracy: 0.8056\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1417 - accuracy: 0.9475 - val_loss: 0.4770 - val_accuracy: 0.7778\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.1445 - accuracy: 0.9383 - val_loss: 0.4794 - val_accuracy: 0.7778\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9630 - val_loss: 0.5053 - val_accuracy: 0.7500\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1306 - accuracy: 0.9537 - val_loss: 0.4895 - val_accuracy: 0.7500\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1040 - accuracy: 0.9568 - val_loss: 0.4823 - val_accuracy: 0.7778\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1277 - accuracy: 0.9506 - val_loss: 0.4997 - val_accuracy: 0.7778\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1073 - accuracy: 0.9599 - val_loss: 0.5214 - val_accuracy: 0.7500\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1528 - accuracy: 0.9321 - val_loss: 0.5146 - val_accuracy: 0.7222\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=600,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20880622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e752b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3034de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using Pysiology. If you use it in your work, please cite:\n",
      "Gabrieli G., Azhari A., Esposito G. (2020) PySiology: A Python Package for Physiological Feature Extraction. In: Esposito A., Faundez-Zanuy M., Morabito F., Pasero E. (eds) Neural Approaches to Dynamics of Signal Exchanges. Smart Innovation, Systems and Technologies, vol 151. Springer, Singapore\n"
     ]
    }
   ],
   "source": [
    "import pysiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6181765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_windows=[]\n",
    "for i in range(len(x_train)):\n",
    "    x_train_tmp=[]\n",
    "    c=[]\n",
    "    for j in range (len(x_train[i])):\n",
    "        x_train_tmp.append(pd.Series(x_train[i][j]).item())\n",
    "    c.append(pd.Series(x_train_tmp))\n",
    "    emg_windows.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01275e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_array=[]\n",
    "for j in range (0,324):\n",
    "    emg_array_tmp=[]\n",
    "    for i in range(0,1600):#emg_windows[0][0][1600]\n",
    "        emg_array_tmp.append(emg_windows[j][0][i])\n",
    "    emg_array.append(emg_array_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5cf05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praahas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pysiology\\electromyography.py:690: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(len(emg_array)):\n",
    "    #result_temp=[]\n",
    "    result.append(pysiology.electromyography.analyzeEMG(emg_array[i],200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c83dd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDomainFeatures=[]\n",
    "for i in range(len(result)):\n",
    "    timeDomainFeatures.append(result[i][\"TimeDomain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd05e118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IEMG': 9645.68572965534,\n",
       " 'MAV': 6.028553581034588,\n",
       " 'MAV1': 5.396637471912048,\n",
       " 'MAV2': 5.459204650619827,\n",
       " 'SSI': 151598.21208958008,\n",
       " 'VAR': 94.8081376420138,\n",
       " 'TM3': 21.54692195498255,\n",
       " 'TM4': 55182.65744649245,\n",
       " 'TM5': 17935.980534590817,\n",
       " 'LOG': 415.1141659341805,\n",
       " 'RMS': 9.73390376755326,\n",
       " 'WL': 11795.364276178927,\n",
       " 'AAC': 7.372102672611829,\n",
       " 'DASDV': 138.0237504954308,\n",
       " 'AFB': 0.0042718237902340805,\n",
       " 'ZC': 692,\n",
       " 'MYOP': 0.99125,\n",
       " 'WAMP': 800,\n",
       " 'SSC': 400,\n",
       " 'MAVSLPk': [5.95614806941166,\n",
       "  9.716618286406051,\n",
       "  2.4239399526202847,\n",
       "  0.14126725788869238],\n",
       " 'HIST': {1: {'ZC': 76, 'WAMP': 85},\n",
       "  2: {'ZC': 81, 'WAMP': 87},\n",
       "  3: {'ZC': 75, 'WAMP': 91},\n",
       "  4: {'ZC': 69, 'WAMP': 81},\n",
       "  5: {'ZC': 76, 'WAMP': 87},\n",
       "  6: {'ZC': 71, 'WAMP': 92},\n",
       "  7: {'ZC': 81, 'WAMP': 89},\n",
       "  8: {'ZC': 79, 'WAMP': 91},\n",
       "  9: {'ZC': 79, 'WAMP': 88}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeDomainFeatures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c455a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAV=[]\n",
    "MAV1=[]\n",
    "MAV2=[]\n",
    "SSI=[]\n",
    "VAR=[]\n",
    "TM3=[]\n",
    "TM4=[]\n",
    "TM5=[]\n",
    "LOG=[]\n",
    "RMS=[]\n",
    "WL=[]\n",
    "AAC=[]\n",
    "DASDV=[]\n",
    "AFB=[]\n",
    "ZC=[]\n",
    "MYOP=[]\n",
    "WAMP=[]\n",
    "SSC=[]\n",
    "for i in range(len(timeDomainFeatures)):\n",
    "    MAV.append(timeDomainFeatures[i][\"MAV\"])\n",
    "    MAV1.append(timeDomainFeatures[i][\"MAV1\"])\n",
    "    MAV2.append(timeDomainFeatures[i][\"MAV2\"])\n",
    "    SSI.append(timeDomainFeatures[i][\"SSI\"])\n",
    "    VAR.append(timeDomainFeatures[i][\"VAR\"])\n",
    "    TM3.append(timeDomainFeatures[i][\"TM3\"])\n",
    "    TM4.append(timeDomainFeatures[i][\"TM4\"])\n",
    "    TM5.append(timeDomainFeatures[i][\"TM5\"])\n",
    "    LOG.append(timeDomainFeatures[i][\"LOG\"])\n",
    "    RMS.append(timeDomainFeatures[i][\"RMS\"])\n",
    "    WL.append(timeDomainFeatures[i][\"WL\"])\n",
    "    AAC.append(timeDomainFeatures[i][\"AAC\"])\n",
    "    DASDV.append(timeDomainFeatures[i][\"DASDV\"])\n",
    "    AFB.append(timeDomainFeatures[i][\"AFB\"]) \n",
    "    ZC.append(timeDomainFeatures[i][\"ZC\"])\n",
    "    MYOP.append(timeDomainFeatures[i][\"MYOP\"])\n",
    "    WAMP.append(timeDomainFeatures[i][\"WAMP\"])\n",
    "    SSC.append(timeDomainFeatures[i][\"SSC\"])\n",
    "\n",
    "TDF=[]\n",
    "for i in range (324):\n",
    "    tmp=[]\n",
    "    tmp.append(MAV[i])\n",
    "    tmp.append(MAV1[i])\n",
    "    tmp.append(MAV2[i])\n",
    "    tmp.append(SSI[i])\n",
    "    tmp.append(VAR[i])\n",
    "    tmp.append(TM3[i])\n",
    "    tmp.append(TM4[i])\n",
    "    tmp.append(TM5[i])\n",
    "    tmp.append(LOG[i])\n",
    "    tmp.append(RMS[i])\n",
    "    tmp.append(WL[i])\n",
    "    tmp.append(AAC[i])\n",
    "    tmp.append(AFB[i])\n",
    "    tmp.append(MYOP[i])\n",
    "    tmp.append(WAMP[i])\n",
    "    tmp.append(SSC[i])\n",
    "    TDF.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c9e05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_train_df=pd.DataFrame(np.array(TDF),columns=['MAV','MAV1','MAV2','SSI','VAR','TM3','TM4','TM5','LOG','RMS','WL','AAC','AFB','MYOP','WAMP','SSC'])\n",
    "emg_train_df['label']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c95918a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAV</th>\n",
       "      <th>MAV1</th>\n",
       "      <th>MAV2</th>\n",
       "      <th>SSI</th>\n",
       "      <th>VAR</th>\n",
       "      <th>TM3</th>\n",
       "      <th>TM4</th>\n",
       "      <th>TM5</th>\n",
       "      <th>LOG</th>\n",
       "      <th>RMS</th>\n",
       "      <th>WL</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AFB</th>\n",
       "      <th>MYOP</th>\n",
       "      <th>WAMP</th>\n",
       "      <th>SSC</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.028554</td>\n",
       "      <td>5.396637</td>\n",
       "      <td>5.459205</td>\n",
       "      <td>151598.212090</td>\n",
       "      <td>94.808138</td>\n",
       "      <td>21.546922</td>\n",
       "      <td>55182.657446</td>\n",
       "      <td>1.793598e+04</td>\n",
       "      <td>415.114166</td>\n",
       "      <td>9.733904</td>\n",
       "      <td>11795.364276</td>\n",
       "      <td>7.372103</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.991250</td>\n",
       "      <td>800.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.445955</td>\n",
       "      <td>0.334276</td>\n",
       "      <td>0.220175</td>\n",
       "      <td>503.528588</td>\n",
       "      <td>0.314902</td>\n",
       "      <td>0.033620</td>\n",
       "      <td>0.298232</td>\n",
       "      <td>1.141219e-01</td>\n",
       "      <td>1.561982</td>\n",
       "      <td>0.560986</td>\n",
       "      <td>804.475959</td>\n",
       "      <td>0.502797</td>\n",
       "      <td>0.064823</td>\n",
       "      <td>0.988125</td>\n",
       "      <td>786.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.227068</td>\n",
       "      <td>6.932964</td>\n",
       "      <td>7.130886</td>\n",
       "      <td>295751.717103</td>\n",
       "      <td>184.960423</td>\n",
       "      <td>381.660100</td>\n",
       "      <td>248257.186842</td>\n",
       "      <td>2.410998e+06</td>\n",
       "      <td>3740.849845</td>\n",
       "      <td>13.595765</td>\n",
       "      <td>15796.333014</td>\n",
       "      <td>9.872708</td>\n",
       "      <td>0.199162</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>809.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500328</td>\n",
       "      <td>0.372946</td>\n",
       "      <td>0.245549</td>\n",
       "      <td>618.078692</td>\n",
       "      <td>0.386541</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.423909</td>\n",
       "      <td>5.283207e-02</td>\n",
       "      <td>1.649263</td>\n",
       "      <td>0.621530</td>\n",
       "      <td>965.817473</td>\n",
       "      <td>0.603636</td>\n",
       "      <td>0.057677</td>\n",
       "      <td>0.986250</td>\n",
       "      <td>782.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.263915</td>\n",
       "      <td>7.492572</td>\n",
       "      <td>7.681053</td>\n",
       "      <td>276144.372415</td>\n",
       "      <td>172.698169</td>\n",
       "      <td>176.426267</td>\n",
       "      <td>146375.826130</td>\n",
       "      <td>3.782570e+05</td>\n",
       "      <td>3881.258550</td>\n",
       "      <td>13.137360</td>\n",
       "      <td>16819.742652</td>\n",
       "      <td>10.512339</td>\n",
       "      <td>0.073259</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>799.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1.918155</td>\n",
       "      <td>1.486753</td>\n",
       "      <td>0.938698</td>\n",
       "      <td>19490.269426</td>\n",
       "      <td>12.189037</td>\n",
       "      <td>46.261887</td>\n",
       "      <td>5006.243557</td>\n",
       "      <td>3.959451e+04</td>\n",
       "      <td>6.808383</td>\n",
       "      <td>3.490189</td>\n",
       "      <td>3663.324505</td>\n",
       "      <td>2.289578</td>\n",
       "      <td>0.098869</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>826.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>5.243188</td>\n",
       "      <td>4.323372</td>\n",
       "      <td>4.061072</td>\n",
       "      <td>121316.723796</td>\n",
       "      <td>75.870371</td>\n",
       "      <td>148.458033</td>\n",
       "      <td>51939.184095</td>\n",
       "      <td>6.344994e+05</td>\n",
       "      <td>189.272521</td>\n",
       "      <td>8.707638</td>\n",
       "      <td>10058.276142</td>\n",
       "      <td>6.286423</td>\n",
       "      <td>0.077266</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>801.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>7.825696</td>\n",
       "      <td>6.816315</td>\n",
       "      <td>5.894675</td>\n",
       "      <td>237129.416033</td>\n",
       "      <td>148.298572</td>\n",
       "      <td>315.836515</td>\n",
       "      <td>122365.419662</td>\n",
       "      <td>6.374017e+05</td>\n",
       "      <td>2504.128978</td>\n",
       "      <td>12.173984</td>\n",
       "      <td>14773.764424</td>\n",
       "      <td>9.233603</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>767.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.819504</td>\n",
       "      <td>0.610763</td>\n",
       "      <td>0.311605</td>\n",
       "      <td>2331.121102</td>\n",
       "      <td>1.457862</td>\n",
       "      <td>0.257762</td>\n",
       "      <td>19.377401</td>\n",
       "      <td>4.073800e+00</td>\n",
       "      <td>2.269375</td>\n",
       "      <td>1.207042</td>\n",
       "      <td>1570.862707</td>\n",
       "      <td>0.981789</td>\n",
       "      <td>-0.011578</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>800.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>6.632531</td>\n",
       "      <td>5.981619</td>\n",
       "      <td>5.289753</td>\n",
       "      <td>181915.724438</td>\n",
       "      <td>113.768433</td>\n",
       "      <td>220.974810</td>\n",
       "      <td>75122.402694</td>\n",
       "      <td>3.325812e+05</td>\n",
       "      <td>759.402057</td>\n",
       "      <td>10.662895</td>\n",
       "      <td>12370.711760</td>\n",
       "      <td>7.731695</td>\n",
       "      <td>-0.056311</td>\n",
       "      <td>0.991250</td>\n",
       "      <td>793.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MAV      MAV1      MAV2            SSI         VAR         TM3  \\\n",
       "0    6.028554  5.396637  5.459205  151598.212090   94.808138   21.546922   \n",
       "1    0.445955  0.334276  0.220175     503.528588    0.314902    0.033620   \n",
       "2    8.227068  6.932964  7.130886  295751.717103  184.960423  381.660100   \n",
       "3    0.500328  0.372946  0.245549     618.078692    0.386541    0.022242   \n",
       "4    8.263915  7.492572  7.681053  276144.372415  172.698169  176.426267   \n",
       "..        ...       ...       ...            ...         ...         ...   \n",
       "319  1.918155  1.486753  0.938698   19490.269426   12.189037   46.261887   \n",
       "320  5.243188  4.323372  4.061072  121316.723796   75.870371  148.458033   \n",
       "321  7.825696  6.816315  5.894675  237129.416033  148.298572  315.836515   \n",
       "322  0.819504  0.610763  0.311605    2331.121102    1.457862    0.257762   \n",
       "323  6.632531  5.981619  5.289753  181915.724438  113.768433  220.974810   \n",
       "\n",
       "               TM4           TM5          LOG        RMS            WL  \\\n",
       "0     55182.657446  1.793598e+04   415.114166   9.733904  11795.364276   \n",
       "1         0.298232  1.141219e-01     1.561982   0.560986    804.475959   \n",
       "2    248257.186842  2.410998e+06  3740.849845  13.595765  15796.333014   \n",
       "3         0.423909  5.283207e-02     1.649263   0.621530    965.817473   \n",
       "4    146375.826130  3.782570e+05  3881.258550  13.137360  16819.742652   \n",
       "..             ...           ...          ...        ...           ...   \n",
       "319    5006.243557  3.959451e+04     6.808383   3.490189   3663.324505   \n",
       "320   51939.184095  6.344994e+05   189.272521   8.707638  10058.276142   \n",
       "321  122365.419662  6.374017e+05  2504.128978  12.173984  14773.764424   \n",
       "322      19.377401  4.073800e+00     2.269375   1.207042   1570.862707   \n",
       "323   75122.402694  3.325812e+05   759.402057  10.662895  12370.711760   \n",
       "\n",
       "           AAC       AFB      MYOP   WAMP    SSC  label  \n",
       "0     7.372103  0.004272  0.991250  800.0  400.0      1  \n",
       "1     0.502797  0.064823  0.988125  786.0  338.0      4  \n",
       "2     9.872708  0.199162  0.990625  809.0  378.0      1  \n",
       "3     0.603636  0.057677  0.986250  782.0  361.0      4  \n",
       "4    10.512339  0.073259  0.991875  799.0  371.0      0  \n",
       "..         ...       ...       ...    ...    ...    ...  \n",
       "319   2.289578  0.098869  0.993750  826.0  360.0      3  \n",
       "320   6.286423  0.077266  0.993750  801.0  372.0      2  \n",
       "321   9.233603 -0.046064  0.995625  767.0  381.0      1  \n",
       "322   0.981789 -0.011578  0.991875  800.0  377.0      4  \n",
       "323   7.731695 -0.056311  0.991250  793.0  368.0      1  \n",
       "\n",
       "[324 rows x 17 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21d97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_windows=[]\n",
    "for i in range(len(x_test)):\n",
    "    x_test_tmp=[]\n",
    "    c=[]\n",
    "    for j in range (len(x_test[i])):\n",
    "        x_test_tmp.append(pd.Series(x_test[i][j]).item())\n",
    "    c.append(pd.Series(x_test_tmp))\n",
    "    emg_windows.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74cdbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_array=[]\n",
    "for j in range (0,36):\n",
    "    emg_array_tmp=[]\n",
    "    for i in range(0,1600):#emg_windows[0][0][1600]\n",
    "        emg_array_tmp.append(emg_windows[j][0][i])\n",
    "    emg_array.append(emg_array_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d74eb5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praahas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pysiology\\electromyography.py:690: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(len(emg_array)):\n",
    "    #result_temp=[]\n",
    "    result.append(pysiology.electromyography.analyzeEMG(emg_array[i],200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8161a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDomainFeatures=[]\n",
    "for i in range(len(result)):\n",
    "    timeDomainFeatures.append(result[i][\"TimeDomain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48e660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAV=[]\n",
    "MAV1=[]\n",
    "MAV2=[]\n",
    "SSI=[]\n",
    "VAR=[]\n",
    "TM3=[]\n",
    "TM4=[]\n",
    "TM5=[]\n",
    "LOG=[]\n",
    "RMS=[]\n",
    "WL=[]\n",
    "AAC=[]\n",
    "DASDV=[]\n",
    "AFB=[]\n",
    "ZC=[]\n",
    "MYOP=[]\n",
    "WAMP=[]\n",
    "SSC=[]\n",
    "for i in range(len(timeDomainFeatures)):\n",
    "    MAV.append(timeDomainFeatures[i][\"MAV\"])\n",
    "    MAV1.append(timeDomainFeatures[i][\"MAV1\"])\n",
    "    MAV2.append(timeDomainFeatures[i][\"MAV2\"])\n",
    "    SSI.append(timeDomainFeatures[i][\"SSI\"])\n",
    "    VAR.append(timeDomainFeatures[i][\"VAR\"])\n",
    "    TM3.append(timeDomainFeatures[i][\"TM3\"])\n",
    "    TM4.append(timeDomainFeatures[i][\"TM4\"])\n",
    "    TM5.append(timeDomainFeatures[i][\"TM5\"])\n",
    "    LOG.append(timeDomainFeatures[i][\"LOG\"])\n",
    "    RMS.append(timeDomainFeatures[i][\"RMS\"])\n",
    "    WL.append(timeDomainFeatures[i][\"WL\"])\n",
    "    AAC.append(timeDomainFeatures[i][\"AAC\"])\n",
    "    DASDV.append(timeDomainFeatures[i][\"DASDV\"])\n",
    "    AFB.append(timeDomainFeatures[i][\"AFB\"]) \n",
    "    ZC.append(timeDomainFeatures[i][\"ZC\"])\n",
    "    MYOP.append(timeDomainFeatures[i][\"MYOP\"])\n",
    "    WAMP.append(timeDomainFeatures[i][\"WAMP\"])\n",
    "    SSC.append(timeDomainFeatures[i][\"SSC\"])\n",
    "    \n",
    "TDF=[]\n",
    "for i in range (36):\n",
    "    tmp=[]\n",
    "    tmp.append(MAV[i])\n",
    "    tmp.append(MAV1[i])\n",
    "    tmp.append(MAV2[i])\n",
    "    tmp.append(SSI[i])\n",
    "    tmp.append(VAR[i])\n",
    "    tmp.append(TM3[i])\n",
    "    tmp.append(TM4[i])\n",
    "    tmp.append(TM5[i])\n",
    "    tmp.append(LOG[i])\n",
    "    tmp.append(RMS[i])\n",
    "    tmp.append(WL[i])\n",
    "    tmp.append(AAC[i])\n",
    "    tmp.append(AFB[i])\n",
    "    tmp.append(MYOP[i])\n",
    "    tmp.append(WAMP[i])\n",
    "    tmp.append(SSC[i])\n",
    "    TDF.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17499b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_test_df=pd.DataFrame(np.array(TDF),columns=['MAV','MAV1','MAV2','SSI','VAR','TM3','TM4','TM5','LOG','RMS','WL','AAC','AFB','MYOP','WAMP','SSC'])\n",
    "emg_test_df['label']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dea329d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAV</th>\n",
       "      <th>MAV1</th>\n",
       "      <th>MAV2</th>\n",
       "      <th>SSI</th>\n",
       "      <th>VAR</th>\n",
       "      <th>TM3</th>\n",
       "      <th>TM4</th>\n",
       "      <th>TM5</th>\n",
       "      <th>LOG</th>\n",
       "      <th>RMS</th>\n",
       "      <th>WL</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AFB</th>\n",
       "      <th>MYOP</th>\n",
       "      <th>WAMP</th>\n",
       "      <th>SSC</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.978625</td>\n",
       "      <td>3.974449</td>\n",
       "      <td>2.893142</td>\n",
       "      <td>114027.713033</td>\n",
       "      <td>71.311891</td>\n",
       "      <td>58.642978</td>\n",
       "      <td>69504.719588</td>\n",
       "      <td>2.923444e+05</td>\n",
       "      <td>145.274445</td>\n",
       "      <td>8.441997</td>\n",
       "      <td>9660.621178</td>\n",
       "      <td>6.037888</td>\n",
       "      <td>-0.031167</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>791.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.265882</td>\n",
       "      <td>7.128799</td>\n",
       "      <td>4.512133</td>\n",
       "      <td>272666.114361</td>\n",
       "      <td>170.522898</td>\n",
       "      <td>680.749669</td>\n",
       "      <td>166013.812074</td>\n",
       "      <td>2.127481e+06</td>\n",
       "      <td>3888.903345</td>\n",
       "      <td>13.054360</td>\n",
       "      <td>16210.412993</td>\n",
       "      <td>10.131508</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>819.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.433739</td>\n",
       "      <td>0.324455</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>477.294491</td>\n",
       "      <td>0.298496</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.280304</td>\n",
       "      <td>1.825565e-02</td>\n",
       "      <td>1.543016</td>\n",
       "      <td>0.546177</td>\n",
       "      <td>768.089783</td>\n",
       "      <td>0.480056</td>\n",
       "      <td>0.016663</td>\n",
       "      <td>0.985625</td>\n",
       "      <td>791.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.038520</td>\n",
       "      <td>4.851217</td>\n",
       "      <td>3.384443</td>\n",
       "      <td>121242.869941</td>\n",
       "      <td>75.824184</td>\n",
       "      <td>71.554331</td>\n",
       "      <td>38782.670347</td>\n",
       "      <td>5.806863e+03</td>\n",
       "      <td>419.272178</td>\n",
       "      <td>8.704987</td>\n",
       "      <td>11457.774993</td>\n",
       "      <td>7.161109</td>\n",
       "      <td>-0.026828</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>814.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.819590</td>\n",
       "      <td>1.459019</td>\n",
       "      <td>0.810232</td>\n",
       "      <td>17634.982553</td>\n",
       "      <td>11.028757</td>\n",
       "      <td>35.313764</td>\n",
       "      <td>3895.205926</td>\n",
       "      <td>5.868945e+04</td>\n",
       "      <td>6.169327</td>\n",
       "      <td>3.319919</td>\n",
       "      <td>3441.876162</td>\n",
       "      <td>2.151173</td>\n",
       "      <td>-0.069872</td>\n",
       "      <td>0.994375</td>\n",
       "      <td>785.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.850457</td>\n",
       "      <td>6.479890</td>\n",
       "      <td>6.459640</td>\n",
       "      <td>207144.551265</td>\n",
       "      <td>129.546311</td>\n",
       "      <td>263.123023</td>\n",
       "      <td>114055.031908</td>\n",
       "      <td>1.198187e+06</td>\n",
       "      <td>944.312048</td>\n",
       "      <td>11.378284</td>\n",
       "      <td>13200.370371</td>\n",
       "      <td>8.250231</td>\n",
       "      <td>-0.109561</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>796.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.409426</td>\n",
       "      <td>2.001304</td>\n",
       "      <td>1.815818</td>\n",
       "      <td>20846.036196</td>\n",
       "      <td>13.036921</td>\n",
       "      <td>5.750434</td>\n",
       "      <td>966.810578</td>\n",
       "      <td>1.562784e+03</td>\n",
       "      <td>11.127568</td>\n",
       "      <td>3.609539</td>\n",
       "      <td>4595.582294</td>\n",
       "      <td>2.872239</td>\n",
       "      <td>0.222998</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>802.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.005559</td>\n",
       "      <td>7.755312</td>\n",
       "      <td>7.879936</td>\n",
       "      <td>336485.702341</td>\n",
       "      <td>210.435086</td>\n",
       "      <td>426.844472</td>\n",
       "      <td>267849.644180</td>\n",
       "      <td>8.287844e+05</td>\n",
       "      <td>8148.250458</td>\n",
       "      <td>14.501847</td>\n",
       "      <td>17271.325788</td>\n",
       "      <td>10.794579</td>\n",
       "      <td>0.160483</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>793.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.482309</td>\n",
       "      <td>0.358968</td>\n",
       "      <td>0.236370</td>\n",
       "      <td>579.926793</td>\n",
       "      <td>0.362681</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.380368</td>\n",
       "      <td>1.831842e-02</td>\n",
       "      <td>1.619809</td>\n",
       "      <td>0.602042</td>\n",
       "      <td>922.323961</td>\n",
       "      <td>0.576452</td>\n",
       "      <td>-0.039900</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>782.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.535321</td>\n",
       "      <td>0.396384</td>\n",
       "      <td>0.267385</td>\n",
       "      <td>1019.705600</td>\n",
       "      <td>0.637715</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>27.291173</td>\n",
       "      <td>3.450724e+00</td>\n",
       "      <td>1.707996</td>\n",
       "      <td>0.798321</td>\n",
       "      <td>1040.400386</td>\n",
       "      <td>0.650250</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.991250</td>\n",
       "      <td>788.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.219571</td>\n",
       "      <td>7.192329</td>\n",
       "      <td>6.605503</td>\n",
       "      <td>258966.606879</td>\n",
       "      <td>161.955351</td>\n",
       "      <td>452.843488</td>\n",
       "      <td>153792.796579</td>\n",
       "      <td>1.593124e+06</td>\n",
       "      <td>3712.909209</td>\n",
       "      <td>12.722190</td>\n",
       "      <td>15521.434857</td>\n",
       "      <td>9.700897</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>801.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.283041</td>\n",
       "      <td>0.997274</td>\n",
       "      <td>0.484236</td>\n",
       "      <td>9845.680101</td>\n",
       "      <td>6.157398</td>\n",
       "      <td>10.646454</td>\n",
       "      <td>1649.202294</td>\n",
       "      <td>1.610480e+04</td>\n",
       "      <td>3.607595</td>\n",
       "      <td>2.480635</td>\n",
       "      <td>2451.567932</td>\n",
       "      <td>1.532230</td>\n",
       "      <td>0.034838</td>\n",
       "      <td>0.986250</td>\n",
       "      <td>786.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.309497</td>\n",
       "      <td>5.595583</td>\n",
       "      <td>4.040107</td>\n",
       "      <td>168786.389649</td>\n",
       "      <td>105.557467</td>\n",
       "      <td>143.542837</td>\n",
       "      <td>70089.383134</td>\n",
       "      <td>9.990009e+04</td>\n",
       "      <td>549.768592</td>\n",
       "      <td>10.270905</td>\n",
       "      <td>12462.303447</td>\n",
       "      <td>7.788940</td>\n",
       "      <td>-0.087318</td>\n",
       "      <td>0.994375</td>\n",
       "      <td>806.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.327878</td>\n",
       "      <td>5.581824</td>\n",
       "      <td>3.959085</td>\n",
       "      <td>161532.074181</td>\n",
       "      <td>101.020684</td>\n",
       "      <td>81.138965</td>\n",
       "      <td>57762.430053</td>\n",
       "      <td>1.428308e+05</td>\n",
       "      <td>559.966915</td>\n",
       "      <td>10.047763</td>\n",
       "      <td>11969.028391</td>\n",
       "      <td>7.480643</td>\n",
       "      <td>-0.074361</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>810.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.005336</td>\n",
       "      <td>7.255574</td>\n",
       "      <td>7.444716</td>\n",
       "      <td>261772.542701</td>\n",
       "      <td>163.710158</td>\n",
       "      <td>599.230644</td>\n",
       "      <td>155745.002656</td>\n",
       "      <td>1.972060e+06</td>\n",
       "      <td>2996.906392</td>\n",
       "      <td>12.790928</td>\n",
       "      <td>15722.144986</td>\n",
       "      <td>9.826341</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>811.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.904675</td>\n",
       "      <td>5.349880</td>\n",
       "      <td>4.127238</td>\n",
       "      <td>158339.170848</td>\n",
       "      <td>99.023872</td>\n",
       "      <td>25.039137</td>\n",
       "      <td>79248.359163</td>\n",
       "      <td>4.251619e+05</td>\n",
       "      <td>366.748078</td>\n",
       "      <td>9.947964</td>\n",
       "      <td>11704.332434</td>\n",
       "      <td>7.315208</td>\n",
       "      <td>-0.102961</td>\n",
       "      <td>0.991250</td>\n",
       "      <td>782.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.958720</td>\n",
       "      <td>5.249733</td>\n",
       "      <td>3.813344</td>\n",
       "      <td>173825.332893</td>\n",
       "      <td>108.708776</td>\n",
       "      <td>254.249293</td>\n",
       "      <td>135859.570150</td>\n",
       "      <td>2.175141e+06</td>\n",
       "      <td>387.114313</td>\n",
       "      <td>10.423091</td>\n",
       "      <td>11497.235953</td>\n",
       "      <td>7.185772</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>798.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.084283</td>\n",
       "      <td>6.230795</td>\n",
       "      <td>4.283775</td>\n",
       "      <td>203045.064209</td>\n",
       "      <td>126.982529</td>\n",
       "      <td>88.277444</td>\n",
       "      <td>87283.792888</td>\n",
       "      <td>1.495661e+05</td>\n",
       "      <td>1193.068080</td>\n",
       "      <td>11.265130</td>\n",
       "      <td>13802.579218</td>\n",
       "      <td>8.626612</td>\n",
       "      <td>0.057805</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>809.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.510952</td>\n",
       "      <td>1.213617</td>\n",
       "      <td>0.922201</td>\n",
       "      <td>11341.081320</td>\n",
       "      <td>7.092609</td>\n",
       "      <td>4.335676</td>\n",
       "      <td>2227.752300</td>\n",
       "      <td>1.089475e+04</td>\n",
       "      <td>4.531044</td>\n",
       "      <td>2.662363</td>\n",
       "      <td>2989.560445</td>\n",
       "      <td>1.868475</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>0.993125</td>\n",
       "      <td>814.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.257661</td>\n",
       "      <td>0.961788</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>4887.765888</td>\n",
       "      <td>3.056764</td>\n",
       "      <td>0.189619</td>\n",
       "      <td>52.231798</td>\n",
       "      <td>2.545034e+00</td>\n",
       "      <td>3.517184</td>\n",
       "      <td>1.747814</td>\n",
       "      <td>2496.452746</td>\n",
       "      <td>1.560283</td>\n",
       "      <td>0.051869</td>\n",
       "      <td>0.994375</td>\n",
       "      <td>774.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.712844</td>\n",
       "      <td>4.926911</td>\n",
       "      <td>3.203318</td>\n",
       "      <td>144891.873134</td>\n",
       "      <td>90.614054</td>\n",
       "      <td>172.071222</td>\n",
       "      <td>52971.868406</td>\n",
       "      <td>2.515916e+05</td>\n",
       "      <td>302.730899</td>\n",
       "      <td>9.516166</td>\n",
       "      <td>10901.279730</td>\n",
       "      <td>6.813300</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>792.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.466773</td>\n",
       "      <td>0.348923</td>\n",
       "      <td>0.229564</td>\n",
       "      <td>556.082146</td>\n",
       "      <td>0.347769</td>\n",
       "      <td>0.007609</td>\n",
       "      <td>0.373603</td>\n",
       "      <td>1.899627e-02</td>\n",
       "      <td>1.594840</td>\n",
       "      <td>0.589535</td>\n",
       "      <td>868.303837</td>\n",
       "      <td>0.542690</td>\n",
       "      <td>-0.004770</td>\n",
       "      <td>0.983750</td>\n",
       "      <td>776.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.090125</td>\n",
       "      <td>1.816210</td>\n",
       "      <td>1.697137</td>\n",
       "      <td>15839.280272</td>\n",
       "      <td>9.905741</td>\n",
       "      <td>7.346345</td>\n",
       "      <td>531.205231</td>\n",
       "      <td>1.550370e+03</td>\n",
       "      <td>8.085927</td>\n",
       "      <td>3.146355</td>\n",
       "      <td>3985.578401</td>\n",
       "      <td>2.490987</td>\n",
       "      <td>-0.064722</td>\n",
       "      <td>0.988750</td>\n",
       "      <td>799.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.051792</td>\n",
       "      <td>6.854360</td>\n",
       "      <td>4.415912</td>\n",
       "      <td>248980.596834</td>\n",
       "      <td>155.710192</td>\n",
       "      <td>406.101808</td>\n",
       "      <td>130367.256198</td>\n",
       "      <td>8.496530e+05</td>\n",
       "      <td>3139.416312</td>\n",
       "      <td>12.474489</td>\n",
       "      <td>15683.652135</td>\n",
       "      <td>9.802283</td>\n",
       "      <td>-0.004511</td>\n",
       "      <td>0.991250</td>\n",
       "      <td>815.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.852142</td>\n",
       "      <td>5.961499</td>\n",
       "      <td>3.962188</td>\n",
       "      <td>187375.314550</td>\n",
       "      <td>117.182811</td>\n",
       "      <td>162.721926</td>\n",
       "      <td>77277.757304</td>\n",
       "      <td>1.998817e+05</td>\n",
       "      <td>945.905095</td>\n",
       "      <td>10.821718</td>\n",
       "      <td>13249.402080</td>\n",
       "      <td>8.280876</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>819.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.877598</td>\n",
       "      <td>5.125206</td>\n",
       "      <td>1.595357</td>\n",
       "      <td>245663.795755</td>\n",
       "      <td>153.635895</td>\n",
       "      <td>302.905135</td>\n",
       "      <td>201151.399315</td>\n",
       "      <td>6.243705e+05</td>\n",
       "      <td>970.293374</td>\n",
       "      <td>12.391121</td>\n",
       "      <td>13368.768064</td>\n",
       "      <td>8.355480</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>806.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.323105</td>\n",
       "      <td>2.020344</td>\n",
       "      <td>1.580397</td>\n",
       "      <td>21507.007166</td>\n",
       "      <td>13.450286</td>\n",
       "      <td>8.925427</td>\n",
       "      <td>1358.774291</td>\n",
       "      <td>1.348164e+03</td>\n",
       "      <td>10.207315</td>\n",
       "      <td>3.666317</td>\n",
       "      <td>4469.154587</td>\n",
       "      <td>2.793222</td>\n",
       "      <td>-0.046280</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>785.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.117405</td>\n",
       "      <td>6.182949</td>\n",
       "      <td>4.031482</td>\n",
       "      <td>215775.736509</td>\n",
       "      <td>134.944175</td>\n",
       "      <td>118.072613</td>\n",
       "      <td>111956.255989</td>\n",
       "      <td>5.604136e+05</td>\n",
       "      <td>1233.245438</td>\n",
       "      <td>11.612917</td>\n",
       "      <td>13812.590519</td>\n",
       "      <td>8.632869</td>\n",
       "      <td>-0.040403</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>799.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.441396</td>\n",
       "      <td>7.613522</td>\n",
       "      <td>7.628462</td>\n",
       "      <td>284192.078210</td>\n",
       "      <td>177.731131</td>\n",
       "      <td>94.713178</td>\n",
       "      <td>177335.001886</td>\n",
       "      <td>3.453394e+05</td>\n",
       "      <td>4635.019343</td>\n",
       "      <td>13.327417</td>\n",
       "      <td>16878.039041</td>\n",
       "      <td>10.548774</td>\n",
       "      <td>0.058990</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>804.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.472794</td>\n",
       "      <td>1.135806</td>\n",
       "      <td>0.565460</td>\n",
       "      <td>11644.727806</td>\n",
       "      <td>7.282506</td>\n",
       "      <td>4.105349</td>\n",
       "      <td>2837.285956</td>\n",
       "      <td>1.022767e+03</td>\n",
       "      <td>4.361406</td>\n",
       "      <td>2.697768</td>\n",
       "      <td>2671.914551</td>\n",
       "      <td>1.669947</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.988750</td>\n",
       "      <td>804.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.044672</td>\n",
       "      <td>7.276011</td>\n",
       "      <td>5.658902</td>\n",
       "      <td>267285.627493</td>\n",
       "      <td>167.157991</td>\n",
       "      <td>247.195685</td>\n",
       "      <td>160351.952105</td>\n",
       "      <td>6.476239e+05</td>\n",
       "      <td>3117.143121</td>\n",
       "      <td>12.924918</td>\n",
       "      <td>15812.681911</td>\n",
       "      <td>9.882926</td>\n",
       "      <td>-0.104716</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>785.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.398098</td>\n",
       "      <td>2.011376</td>\n",
       "      <td>1.640112</td>\n",
       "      <td>21755.252718</td>\n",
       "      <td>13.605536</td>\n",
       "      <td>0.990490</td>\n",
       "      <td>1306.182240</td>\n",
       "      <td>1.680775e+03</td>\n",
       "      <td>11.002227</td>\n",
       "      <td>3.687415</td>\n",
       "      <td>4630.845457</td>\n",
       "      <td>2.894278</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>797.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.435123</td>\n",
       "      <td>6.576412</td>\n",
       "      <td>4.970865</td>\n",
       "      <td>197521.726630</td>\n",
       "      <td>123.528284</td>\n",
       "      <td>241.211676</td>\n",
       "      <td>71819.413594</td>\n",
       "      <td>4.687191e+05</td>\n",
       "      <td>1694.465495</td>\n",
       "      <td>11.110854</td>\n",
       "      <td>14610.752445</td>\n",
       "      <td>9.131720</td>\n",
       "      <td>-0.060640</td>\n",
       "      <td>0.996250</td>\n",
       "      <td>805.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.921145</td>\n",
       "      <td>1.590413</td>\n",
       "      <td>1.418279</td>\n",
       "      <td>14499.526496</td>\n",
       "      <td>9.067871</td>\n",
       "      <td>5.835849</td>\n",
       "      <td>1042.509381</td>\n",
       "      <td>3.102490e+03</td>\n",
       "      <td>6.828775</td>\n",
       "      <td>3.010349</td>\n",
       "      <td>3793.239860</td>\n",
       "      <td>2.370775</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>791.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.588581</td>\n",
       "      <td>6.177541</td>\n",
       "      <td>6.070952</td>\n",
       "      <td>228697.903425</td>\n",
       "      <td>143.025581</td>\n",
       "      <td>127.635102</td>\n",
       "      <td>116893.923662</td>\n",
       "      <td>6.396450e+04</td>\n",
       "      <td>1975.508919</td>\n",
       "      <td>11.955592</td>\n",
       "      <td>14834.850201</td>\n",
       "      <td>9.271781</td>\n",
       "      <td>0.064906</td>\n",
       "      <td>0.995625</td>\n",
       "      <td>810.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7.860272</td>\n",
       "      <td>7.032290</td>\n",
       "      <td>7.098173</td>\n",
       "      <td>242957.652737</td>\n",
       "      <td>151.943498</td>\n",
       "      <td>390.612673</td>\n",
       "      <td>123654.038914</td>\n",
       "      <td>8.409238e+05</td>\n",
       "      <td>2592.224704</td>\n",
       "      <td>12.322684</td>\n",
       "      <td>15168.346590</td>\n",
       "      <td>9.480217</td>\n",
       "      <td>-0.124009</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>801.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAV      MAV1      MAV2            SSI         VAR         TM3  \\\n",
       "0   4.978625  3.974449  2.893142  114027.713033   71.311891   58.642978   \n",
       "1   8.265882  7.128799  4.512133  272666.114361  170.522898  680.749669   \n",
       "2   0.433739  0.324455  0.220703     477.294491    0.298496    0.006563   \n",
       "3   6.038520  4.851217  3.384443  121242.869941   75.824184   71.554331   \n",
       "4   1.819590  1.459019  0.810232   17634.982553   11.028757   35.313764   \n",
       "5   6.850457  6.479890  6.459640  207144.551265  129.546311  263.123023   \n",
       "6   2.409426  2.001304  1.815818   20846.036196   13.036921    5.750434   \n",
       "7   9.005559  7.755312  7.879936  336485.702341  210.435086  426.844472   \n",
       "8   0.482309  0.358968  0.236370     579.926793    0.362681    0.006216   \n",
       "9   0.535321  0.396384  0.267385    1019.705600    0.637715    0.000042   \n",
       "10  8.219571  7.192329  6.605503  258966.606879  161.955351  452.843488   \n",
       "11  1.283041  0.997274  0.484236    9845.680101    6.157398   10.646454   \n",
       "12  6.309497  5.595583  4.040107  168786.389649  105.557467  143.542837   \n",
       "13  6.327878  5.581824  3.959085  161532.074181  101.020684   81.138965   \n",
       "14  8.005336  7.255574  7.444716  261772.542701  163.710158  599.230644   \n",
       "15  5.904675  5.349880  4.127238  158339.170848   99.023872   25.039137   \n",
       "16  5.958720  5.249733  3.813344  173825.332893  108.708776  254.249293   \n",
       "17  7.084283  6.230795  4.283775  203045.064209  126.982529   88.277444   \n",
       "18  1.510952  1.213617  0.922201   11341.081320    7.092609    4.335676   \n",
       "19  1.257661  0.961788  0.702520    4887.765888    3.056764    0.189619   \n",
       "20  5.712844  4.926911  3.203318  144891.873134   90.614054  172.071222   \n",
       "21  0.466773  0.348923  0.229564     556.082146    0.347769    0.007609   \n",
       "22  2.090125  1.816210  1.697137   15839.280272    9.905741    7.346345   \n",
       "23  8.051792  6.854360  4.415912  248980.596834  155.710192  406.101808   \n",
       "24  6.852142  5.961499  3.962188  187375.314550  117.182811  162.721926   \n",
       "25  6.877598  5.125206  1.595357  245663.795755  153.635895  302.905135   \n",
       "26  2.323105  2.020344  1.580397   21507.007166   13.450286    8.925427   \n",
       "27  7.117405  6.182949  4.031482  215775.736509  134.944175  118.072613   \n",
       "28  8.441396  7.613522  7.628462  284192.078210  177.731131   94.713178   \n",
       "29  1.472794  1.135806  0.565460   11644.727806    7.282506    4.105349   \n",
       "30  8.044672  7.276011  5.658902  267285.627493  167.157991  247.195685   \n",
       "31  2.398098  2.011376  1.640112   21755.252718   13.605536    0.990490   \n",
       "32  7.435123  6.576412  4.970865  197521.726630  123.528284  241.211676   \n",
       "33  1.921145  1.590413  1.418279   14499.526496    9.067871    5.835849   \n",
       "34  7.588581  6.177541  6.070952  228697.903425  143.025581  127.635102   \n",
       "35  7.860272  7.032290  7.098173  242957.652737  151.943498  390.612673   \n",
       "\n",
       "              TM4           TM5          LOG        RMS            WL  \\\n",
       "0    69504.719588  2.923444e+05   145.274445   8.441997   9660.621178   \n",
       "1   166013.812074  2.127481e+06  3888.903345  13.054360  16210.412993   \n",
       "2        0.280304  1.825565e-02     1.543016   0.546177    768.089783   \n",
       "3    38782.670347  5.806863e+03   419.272178   8.704987  11457.774993   \n",
       "4     3895.205926  5.868945e+04     6.169327   3.319919   3441.876162   \n",
       "5   114055.031908  1.198187e+06   944.312048  11.378284  13200.370371   \n",
       "6      966.810578  1.562784e+03    11.127568   3.609539   4595.582294   \n",
       "7   267849.644180  8.287844e+05  8148.250458  14.501847  17271.325788   \n",
       "8        0.380368  1.831842e-02     1.619809   0.602042    922.323961   \n",
       "9       27.291173  3.450724e+00     1.707996   0.798321   1040.400386   \n",
       "10  153792.796579  1.593124e+06  3712.909209  12.722190  15521.434857   \n",
       "11    1649.202294  1.610480e+04     3.607595   2.480635   2451.567932   \n",
       "12   70089.383134  9.990009e+04   549.768592  10.270905  12462.303447   \n",
       "13   57762.430053  1.428308e+05   559.966915  10.047763  11969.028391   \n",
       "14  155745.002656  1.972060e+06  2996.906392  12.790928  15722.144986   \n",
       "15   79248.359163  4.251619e+05   366.748078   9.947964  11704.332434   \n",
       "16  135859.570150  2.175141e+06   387.114313  10.423091  11497.235953   \n",
       "17   87283.792888  1.495661e+05  1193.068080  11.265130  13802.579218   \n",
       "18    2227.752300  1.089475e+04     4.531044   2.662363   2989.560445   \n",
       "19      52.231798  2.545034e+00     3.517184   1.747814   2496.452746   \n",
       "20   52971.868406  2.515916e+05   302.730899   9.516166  10901.279730   \n",
       "21       0.373603  1.899627e-02     1.594840   0.589535    868.303837   \n",
       "22     531.205231  1.550370e+03     8.085927   3.146355   3985.578401   \n",
       "23  130367.256198  8.496530e+05  3139.416312  12.474489  15683.652135   \n",
       "24   77277.757304  1.998817e+05   945.905095  10.821718  13249.402080   \n",
       "25  201151.399315  6.243705e+05   970.293374  12.391121  13368.768064   \n",
       "26    1358.774291  1.348164e+03    10.207315   3.666317   4469.154587   \n",
       "27  111956.255989  5.604136e+05  1233.245438  11.612917  13812.590519   \n",
       "28  177335.001886  3.453394e+05  4635.019343  13.327417  16878.039041   \n",
       "29    2837.285956  1.022767e+03     4.361406   2.697768   2671.914551   \n",
       "30  160351.952105  6.476239e+05  3117.143121  12.924918  15812.681911   \n",
       "31    1306.182240  1.680775e+03    11.002227   3.687415   4630.845457   \n",
       "32   71819.413594  4.687191e+05  1694.465495  11.110854  14610.752445   \n",
       "33    1042.509381  3.102490e+03     6.828775   3.010349   3793.239860   \n",
       "34  116893.923662  6.396450e+04  1975.508919  11.955592  14834.850201   \n",
       "35  123654.038914  8.409238e+05  2592.224704  12.322684  15168.346590   \n",
       "\n",
       "          AAC       AFB      MYOP   WAMP    SSC  label  \n",
       "0    6.037888 -0.031167  0.992500  791.0  357.0      2  \n",
       "1   10.131508  0.005289  0.995000  819.0  351.0      0  \n",
       "2    0.480056  0.016663  0.985625  791.0  333.0      4  \n",
       "3    7.161109 -0.026828  0.998750  814.0  363.0      2  \n",
       "4    2.151173 -0.069872  0.994375  785.0  373.0      3  \n",
       "5    8.250231 -0.109561  0.993750  796.0  369.0      0  \n",
       "6    2.872239  0.222998  0.992500  802.0  355.0      3  \n",
       "7   10.794579  0.160483  0.995625  793.0  345.0      1  \n",
       "8    0.576452 -0.039900  0.987500  782.0  367.0      4  \n",
       "9    0.650250  0.019947  0.991250  788.0  370.0      4  \n",
       "10   9.700897  0.081673  0.992500  801.0  367.0      1  \n",
       "11   1.532230  0.034838  0.986250  786.0  378.0      3  \n",
       "12   7.788940 -0.087318  0.994375  806.0  340.0      0  \n",
       "13   7.480643 -0.074361  0.991875  810.0  354.0      1  \n",
       "14   9.826341  0.054913  0.992500  811.0  354.0      0  \n",
       "15   7.315208 -0.102961  0.991250  782.0  371.0      2  \n",
       "16   7.185772 -0.037538  0.993750  798.0  353.0      2  \n",
       "17   8.626612  0.057805  0.995000  809.0  381.0      0  \n",
       "18   1.868475  0.018861  0.993125  814.0  334.0      3  \n",
       "19   1.560283  0.051869  0.994375  774.0  392.0      4  \n",
       "20   6.813300  0.032242  0.990625  792.0  370.0      1  \n",
       "21   0.542690 -0.004770  0.983750  776.0  370.0      4  \n",
       "22   2.490987 -0.064722  0.988750  799.0  386.0      3  \n",
       "23   9.802283 -0.004511  0.991250  815.0  353.0      1  \n",
       "24   8.280876 -0.003296  0.992500  819.0  361.0      0  \n",
       "25   8.355480 -0.007297  0.993750  806.0  361.0      1  \n",
       "26   2.793222 -0.046280  0.995625  785.0  362.0      3  \n",
       "27   8.632869 -0.040403  0.995625  799.0  359.0      0  \n",
       "28  10.548774  0.058990  0.995000  804.0  406.0      0  \n",
       "29   1.669947  0.072400  0.988750  804.0  364.0      3  \n",
       "30   9.882926 -0.104716  0.995000  785.0  361.0      0  \n",
       "31   2.894278 -0.004426  0.991875  797.0  355.0      3  \n",
       "32   9.131720 -0.060640  0.996250  805.0  361.0      0  \n",
       "33   2.370775  0.021521  0.990625  791.0  375.0      3  \n",
       "34   9.271781  0.064906  0.995625  810.0  369.0      1  \n",
       "35   9.480217 -0.124009  0.993750  801.0  341.0      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a82f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_train_df.to_csv('emg_train.csv')\n",
    "emg_test_df.to_csv('emg_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dd6412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_train_df.append(emg_test_df).reset_index(drop=True, inplace=True)\n",
    "emg_df=emg_train_df.append(emg_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d4d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_df.to_csv(f\"emg_{name}.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca56d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from math import exp\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt #used for visualization purposes in this tutorial.\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, Flatten,Conv2D, MaxPool2D\n",
    "import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "143289bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1600, 32)          64        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 800, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 800, 64)           2112      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 400, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 400, 32)           2080      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 200, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 200, 16)           528       \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 100, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 16)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 100, 8)            136       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 50, 8)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50, 8)             0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 50, 4)             36        \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 25, 4)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 25, 4)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,261\n",
      "Trainable params: 10,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model.keras')\n",
    "model.summary()\n",
    "probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afbbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_control(argument):\n",
    "    switcher = {\n",
    "        0: \"Point\",\n",
    "        1: \"Middle\",\n",
    "        2: \"Grip\",\n",
    "        3: \"Pinch\",\n",
    "        4: \"No Gesture\"\n",
    "    }\n",
    " \n",
    "    # get() method of dictionary data type returns\n",
    "    # value of passed argument if it is present\n",
    "    # in dictionary otherwise second argument will\n",
    "    # be assigned as default value of passed argument\n",
    "    return switcher.get(argument, \"nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e434eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5936b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point\n"
     ]
    }
   ],
   "source": [
    "sample_number=5\n",
    "df = pd.read_csv('GESTURE_DATAFRAME_2.csv')\n",
    "test_sample=df.iloc[sample_number,:-1].to_numpy()\n",
    "test_sample_label=df.iloc[sample_number,-1]\n",
    "test_sample = (np.expand_dims(test_sample,0))\n",
    "predictions_single = probability_model.predict(test_sample)\n",
    "#print(\"Actual Gesture:\",test_sample_label )\n",
    "#print(\"Predicted Gesture:\",np.argmax(predictions_single[0]))\n",
    "\n",
    "if np.argmax(predictions_single[0])==0:\n",
    "    print(\"point\")\n",
    "    angle=[180,0,180,180,180]\n",
    "elif np.argmax(predictions_single[0])==1:\n",
    "     print(\"middle\")\n",
    "        angle=[180,180,0,180,180]\n",
    "elif np.argmax(predictions_single[0])==2:\n",
    "    print(\"grip\")\n",
    "    angle=[180,180,180,180,180]\n",
    "elif np.argmax(predictions_single[0])==3:\n",
    "    print(\"pinch\")\n",
    "    angle=[180,180,180,0,0]\n",
    "else:\n",
    "    print(\"rest\")\n",
    "    angle=[0,0,0,0,0]\n",
    "#kit.servo[0].angle=angle[0]\n",
    "#kit.servo[1].angle=angle[1]\n",
    "#kit.servo[2].angle=angle[2]\n",
    "#kit.servo[3].angle=angle[3]\n",
    "#kit.servo[4].angle=angle[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    #motor_value=input()\n",
    "    #kit.servo[0].angle=int(motor_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
